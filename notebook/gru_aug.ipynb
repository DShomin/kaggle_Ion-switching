{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby, accumulate\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_toolbelt import losses as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"../data/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "train = pd.read_csv('../data/data-without-drift/train_clean.csv')\n",
    "train['filter'] = 0\n",
    "test = pd.read_csv('../data/data-without-drift/test_clean.csv')\n",
    "test['filter'] = 2\n",
    "ts1 = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)\n",
    "\n",
    "ts1['time2'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14 + 1), labels=list(range(14)), include_lowest=True).astype(int)\n",
    "ts1['time2'] = ts1.groupby('time2')['time'].rank( )/500000.\n",
    "\n",
    "np.random.seed(321)\n",
    "ts1['group'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14*125 + 1), labels=list(range(14*125)), include_lowest=True).astype(int)\n",
    "np.random.seed(321)\n",
    "\n",
    "y = ts1.loc[ts1['filter']==0, 'open_channels']\n",
    "group = ts1.loc[ts1['filter']==0, 'group']\n",
    "X = ts1.loc[ts1['filter']==0, 'signal']\n",
    "\n",
    "np.random.seed(321)\n",
    "skf = GroupKFold(n_splits=5)\n",
    "splits = [x for x in skf.split(X, y, group)]\n",
    "\n",
    "use_cols = [col for col in ts1.columns if col not in ['index','filter','group', 'open_channels', 'time', 'time2']]  \n",
    "\n",
    "# Create numpy array of inputs\n",
    "for col in use_cols:\n",
    "    col_mean = ts1[col].mean()\n",
    "    ts1[col] = ts1[col].fillna(col_mean)\n",
    "\n",
    "val_preds_all = np.zeros((ts1[ts1['filter']==0].shape[0], 11))\n",
    "test_preds_all = np.zeros((ts1[ts1['filter']==2].shape[0], 11))\n",
    "\n",
    "groups = ts1.loc[ts1['filter']==0, 'group']\n",
    "times = ts1.loc[ts1['filter']==0, 'time']\n",
    "\n",
    "new_splits = []\n",
    "for sp in splits:\n",
    "    new_split = []\n",
    "    new_split.append(np.unique(groups[sp[0]]))\n",
    "    new_split.append(np.unique(groups[sp[1]]))\n",
    "    new_splits.append(new_split)\n",
    "    \n",
    "trainval = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "test = np.array(list(ts1[ts1['filter']==2].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "trainval_y = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[['open_channels']].values)))\n",
    "\n",
    "gc.collect()\n",
    "# transpose to B x C x L\n",
    "trainval = trainval.transpose((0,2,1))\n",
    "test = test.transpose((0,2,1))\n",
    "\n",
    "trainval_y = trainval_y.reshape(trainval_y.shape[:2])\n",
    "test_y = np.zeros((test.shape[0], trainval_y.shape[1]))\n",
    "\n",
    "trainval = torch.Tensor(trainval)\n",
    "test = torch.Tensor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "class Seq2SeqRnn(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n",
    "            hidden_layers = [100, 200]):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.output_size=output_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, batch_first=True,dropout=0.3)\n",
    "         # Input Layer\n",
    "        if hidden_layers and len(hidden_layers):\n",
    "            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n",
    "\n",
    "            # Hidden Layers\n",
    "            self.hidden_layers = nn.ModuleList(\n",
    "                [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n",
    "            )\n",
    "            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)   \n",
    "\n",
    "            self.intermediate_layer = nn.Linear(hidden_layers[-1], self.input_size)\n",
    "            # output layers\n",
    "            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "           \n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "            self.intermediate_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_siz, self.input_size)\n",
    "            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "\n",
    "        self.activation_fn = torch.relu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        outputs, hidden = self.rnn(x)        \n",
    "\n",
    "        x = self.dropout(self.activation_fn(outputs))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fn(hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# augmentation\n",
    "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
    "    X_new = np.zeros(X.shape)\n",
    "    idx = np.random.permutation(nPerm)\n",
    "    bWhile = True\n",
    "    while bWhile == True:\n",
    "        segs = np.zeros(nPerm+1, dtype=int)\n",
    "        segs[1:-1] = np.sort(np.random.randint(minSegLength, X.shape[0]-minSegLength, nPerm-1))\n",
    "        segs[-1] = X.shape[0]\n",
    "        if np.min(segs[1:]-segs[0:-1]) > minSegLength:\n",
    "            bWhile = False\n",
    "    pp = 0\n",
    "    for ii in range(nPerm):\n",
    "        x_temp = X[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
    "        X_new[pp:pp+len(x_temp),:] = x_temp\n",
    "        pp += len(x_temp)\n",
    "    return(X_new)\n",
    "\n",
    "def DA_Jitter(X, sigma=0.05):\n",
    "    X = X.numpy()\n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    return X+myNoise\n",
    "\n",
    "def DA_Scaling(X, sigma=0.1):\n",
    "    X = X.numpy()\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1,X.shape[1])) # shape=(1,3)\n",
    "    myNoise = np.matmul(np.ones((X.shape[0],1)), scalingFactor)\n",
    "    return X*myNoise\n",
    "\n",
    "class IonDataset(Dataset):\n",
    "    \"\"\"Car dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, training=True, transform=None, flip=0.5, noise_level=0, class_split=0.0, aug_ratio=0.0):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        self.flip = flip\n",
    "        self.noise_level = noise_level\n",
    "        self.class_split = class_split\n",
    "        self.aug_ratio = aug_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        if np.random.rand() < self.aug_ratio:\n",
    "#             data = DA_Permutation(data)\n",
    "            data = DA_Jitter(data)\n",
    "#             data = DA_Scaling(data)\n",
    "            data = torch.Tensor(data)\n",
    "        if np.random.rand() < self.class_split:\n",
    "            data, labels = class_split(data, labels)\n",
    "        if  np.random.rand() < self.noise_level:\n",
    "            data = data * torch.FloatTensor(10000).uniform_(1-self.noise_level, 1+self.noise_level)\n",
    "        if np.random.rand() < self.flip:\n",
    "            data = torch.flip(data, dims=[1])\n",
    "            labels = np.flip(labels, axis=0).copy().astype(int)\n",
    "\n",
    "        return [data, labels.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "train_loss: 2.681285, valid_loss: 2.249479\n",
      "train_f1: 0.050984, valid_f1: 0.020821\n",
      "--- 28.574880361557007 seconds ---\n",
      "Epoch : 1\n",
      "learning_rate: 0.000011938\n",
      "train_loss: 2.372256, valid_loss: 1.866467\n",
      "train_f1: 0.055356, valid_f1: 0.005914\n",
      "--- 28.51059627532959 seconds ---\n",
      "Epoch : 2\n",
      "learning_rate: 0.000044274\n",
      "train_loss: 1.872657, valid_loss: 1.479555\n",
      "train_f1: 0.062681, valid_f1: 0.052634\n",
      "--- 28.57749605178833 seconds ---\n",
      "Epoch : 3\n",
      "learning_rate: 0.000096592\n",
      "train_loss: 1.375500, valid_loss: 0.900697\n",
      "train_f1: 0.069028, valid_f1: 0.058895\n",
      "--- 28.504749536514282 seconds ---\n",
      "Epoch : 4\n",
      "learning_rate: 0.000166599\n",
      "train_loss: 1.019277, valid_loss: 0.738444\n",
      "train_f1: 0.103366, valid_f1: 0.053341\n",
      "--- 28.91235613822937 seconds ---\n",
      "Epoch : 5\n",
      "learning_rate: 0.000251230\n",
      "train_loss: 0.867324, valid_loss: 0.643836\n",
      "train_f1: 0.136559, valid_f1: 0.130850\n",
      "--- 28.338125467300415 seconds ---\n",
      "Epoch : 6\n",
      "learning_rate: 0.000346779\n",
      "train_loss: 0.757551, valid_loss: 0.552627\n",
      "train_f1: 0.194290, valid_f1: 0.304037\n",
      "--- 28.18520188331604 seconds ---\n",
      "Epoch : 7\n",
      "learning_rate: 0.000449060\n",
      "train_loss: 0.637163, valid_loss: 0.423854\n",
      "train_f1: 0.279522, valid_f1: 0.382678\n",
      "--- 28.372053146362305 seconds ---\n",
      "Epoch : 8\n",
      "learning_rate: 0.000553594\n",
      "train_loss: 0.515380, valid_loss: 0.356371\n",
      "train_f1: 0.355667, valid_f1: 0.397352\n",
      "--- 28.41294288635254 seconds ---\n",
      "Epoch : 9\n",
      "learning_rate: 0.000655802\n",
      "train_loss: 0.439467, valid_loss: 0.301136\n",
      "train_f1: 0.404146, valid_f1: 0.485963\n",
      "--- 28.400786876678467 seconds ---\n",
      "Epoch : 10\n",
      "learning_rate: 0.000751209\n",
      "train_loss: 0.369095, valid_loss: 0.233335\n",
      "train_f1: 0.463931, valid_f1: 0.628196\n",
      "--- 28.299415826797485 seconds ---\n",
      "Epoch : 11\n",
      "learning_rate: 0.000835636\n",
      "train_loss: 0.308667, valid_loss: 0.181489\n",
      "train_f1: 0.538076, valid_f1: 0.706570\n",
      "--- 28.09041166305542 seconds ---\n",
      "Epoch : 12\n",
      "learning_rate: 0.000905384\n",
      "train_loss: 0.260513, valid_loss: 0.148711\n",
      "train_f1: 0.602756, valid_f1: 0.776254\n",
      "--- 28.093769311904907 seconds ---\n",
      "Epoch : 13\n",
      "learning_rate: 0.000957400\n",
      "train_loss: 0.227222, valid_loss: 0.129395\n",
      "train_f1: 0.647992, valid_f1: 0.800266\n",
      "--- 28.43177604675293 seconds ---\n",
      "Epoch : 14\n",
      "learning_rate: 0.000989405\n",
      "train_loss: 0.202615, valid_loss: 0.111051\n",
      "train_f1: 0.684129, valid_f1: 0.815044\n",
      "--- 28.51967453956604 seconds ---\n",
      "Epoch : 15\n",
      "learning_rate: 0.001000000\n",
      "train_loss: 0.181778, valid_loss: 0.097729\n",
      "train_f1: 0.715874, valid_f1: 0.818209\n",
      "--- 28.344075918197632 seconds ---\n",
      "Epoch : 16\n",
      "learning_rate: 0.000999860\n",
      "train_loss: 0.167067, valid_loss: 0.089199\n",
      "train_f1: 0.741747, valid_f1: 0.832579\n",
      "--- 28.267796754837036 seconds ---\n",
      "Epoch : 17\n",
      "learning_rate: 0.000999450\n",
      "train_loss: 0.154192, valid_loss: 0.084914\n",
      "train_f1: 0.761821, valid_f1: 0.833272\n",
      "--- 28.292375326156616 seconds ---\n",
      "Epoch : 18\n",
      "learning_rate: 0.000998769\n",
      "train_loss: 0.143479, valid_loss: 0.078695\n",
      "train_f1: 0.777014, valid_f1: 0.837609\n",
      "--- 28.346102952957153 seconds ---\n",
      "Epoch : 19\n",
      "learning_rate: 0.000997818\n",
      "train_loss: 0.135763, valid_loss: 0.076143\n",
      "train_f1: 0.788244, valid_f1: 0.839594\n",
      "--- 28.324644327163696 seconds ---\n",
      "Epoch : 20\n",
      "learning_rate: 0.000996598\n",
      "train_loss: 0.131223, valid_loss: 0.073169\n",
      "train_f1: 0.796314, valid_f1: 0.840805\n",
      "--- 28.47944688796997 seconds ---\n",
      "Epoch : 21\n",
      "learning_rate: 0.000995108\n",
      "train_loss: 0.124697, valid_loss: 0.080839\n",
      "train_f1: 0.804307, valid_f1: 0.831949\n",
      "--- 28.41234064102173 seconds ---\n",
      "Epoch : 22\n",
      "learning_rate: 0.000993351\n",
      "train_loss: 0.121340, valid_loss: 0.070881\n",
      "train_f1: 0.808785, valid_f1: 0.843072\n",
      "--- 27.69088125228882 seconds ---\n",
      "Epoch : 23\n",
      "learning_rate: 0.000991326\n",
      "train_loss: 0.116301, valid_loss: 0.069946\n",
      "train_f1: 0.814819, valid_f1: 0.843906\n",
      "--- 28.29020929336548 seconds ---\n",
      "Epoch : 24\n",
      "learning_rate: 0.000989035\n",
      "train_loss: 0.112759, valid_loss: 0.068992\n",
      "train_f1: 0.820141, valid_f1: 0.844899\n",
      "--- 28.552793979644775 seconds ---\n",
      "Epoch : 25\n",
      "learning_rate: 0.000986480\n",
      "train_loss: 0.110212, valid_loss: 0.068127\n",
      "train_f1: 0.823224, valid_f1: 0.844540\n",
      "--- 28.300817489624023 seconds ---\n",
      "Epoch : 26\n",
      "learning_rate: 0.000983661\n",
      "train_loss: 0.107335, valid_loss: 0.067169\n",
      "train_f1: 0.830136, valid_f1: 0.845573\n",
      "--- 28.379897356033325 seconds ---\n",
      "Epoch : 27\n",
      "learning_rate: 0.000980580\n",
      "train_loss: 0.104425, valid_loss: 0.066999\n",
      "train_f1: 0.835019, valid_f1: 0.845165\n",
      "--- 28.261953592300415 seconds ---\n",
      "Epoch : 28\n",
      "learning_rate: 0.000977239\n",
      "train_loss: 0.102763, valid_loss: 0.070271\n",
      "train_f1: 0.841071, valid_f1: 0.837350\n",
      "--- 28.347954511642456 seconds ---\n",
      "Epoch : 29\n",
      "learning_rate: 0.000973639\n",
      "train_loss: 0.100932, valid_loss: 0.064214\n",
      "train_f1: 0.847003, valid_f1: 0.850139\n",
      "--- 28.41017723083496 seconds ---\n",
      "Epoch : 30\n",
      "learning_rate: 0.000969783\n",
      "train_loss: 0.098377, valid_loss: 0.063551\n",
      "train_f1: 0.852663, valid_f1: 0.911731\n",
      "--- 28.290870666503906 seconds ---\n",
      "Epoch : 31\n",
      "learning_rate: 0.000965673\n",
      "train_loss: 0.097022, valid_loss: 0.062993\n",
      "train_f1: 0.858608, valid_f1: 0.920084\n",
      "--- 28.32108426094055 seconds ---\n",
      "Epoch : 32\n",
      "learning_rate: 0.000961310\n",
      "train_loss: 0.094990, valid_loss: 0.062701\n",
      "train_f1: 0.863328, valid_f1: 0.928705\n",
      "--- 28.407152891159058 seconds ---\n",
      "Epoch : 33\n",
      "learning_rate: 0.000956698\n",
      "train_loss: 0.093740, valid_loss: 0.062704\n",
      "train_f1: 0.867506, valid_f1: 0.928038\n",
      "--- 28.672196865081787 seconds ---\n",
      "Epoch : 34\n",
      "learning_rate: 0.000951838\n",
      "train_loss: 0.092601, valid_loss: 0.061333\n",
      "train_f1: 0.871446, valid_f1: 0.927607\n",
      "--- 28.484272003173828 seconds ---\n",
      "Epoch : 35\n",
      "learning_rate: 0.000946733\n",
      "train_loss: 0.091462, valid_loss: 0.060363\n",
      "train_f1: 0.874970, valid_f1: 0.931960\n",
      "--- 28.519026279449463 seconds ---\n",
      "Epoch : 36\n",
      "learning_rate: 0.000941387\n",
      "train_loss: 0.090104, valid_loss: 0.059940\n",
      "train_f1: 0.877339, valid_f1: 0.934110\n",
      "--- 28.257195949554443 seconds ---\n",
      "Epoch : 37\n",
      "learning_rate: 0.000935802\n",
      "train_loss: 0.088889, valid_loss: 0.059628\n",
      "train_f1: 0.880308, valid_f1: 0.931220\n",
      "--- 28.562962532043457 seconds ---\n",
      "Epoch : 38\n",
      "learning_rate: 0.000929980\n",
      "train_loss: 0.087867, valid_loss: 0.058998\n",
      "train_f1: 0.882313, valid_f1: 0.935176\n",
      "--- 28.300896406173706 seconds ---\n",
      "Epoch : 39\n",
      "learning_rate: 0.000923926\n",
      "train_loss: 0.087734, valid_loss: 0.066169\n",
      "train_f1: 0.882864, valid_f1: 0.906643\n",
      "--- 28.35764741897583 seconds ---\n",
      "Epoch : 40\n",
      "learning_rate: 0.000917642\n",
      "train_loss: 0.089541, valid_loss: 0.059431\n",
      "train_f1: 0.878914, valid_f1: 0.931019\n",
      "--- 28.56486940383911 seconds ---\n",
      "Epoch : 41\n",
      "learning_rate: 0.000911132\n",
      "train_loss: 0.085653, valid_loss: 0.058423\n",
      "train_f1: 0.886915, valid_f1: 0.935503\n",
      "--- 28.40802788734436 seconds ---\n",
      "Epoch : 42\n",
      "learning_rate: 0.000904400\n",
      "train_loss: 0.084418, valid_loss: 0.057685\n",
      "train_f1: 0.888988, valid_f1: 0.933158\n",
      "--- 28.7271625995636 seconds ---\n",
      "Epoch : 43\n",
      "learning_rate: 0.000897448\n",
      "train_loss: 0.083296, valid_loss: 0.057461\n",
      "train_f1: 0.890343, valid_f1: 0.935782\n",
      "--- 28.391727685928345 seconds ---\n",
      "Epoch : 44\n",
      "learning_rate: 0.000890282\n",
      "train_loss: 0.083168, valid_loss: 0.057847\n",
      "train_f1: 0.891603, valid_f1: 0.934251\n",
      "--- 28.884398221969604 seconds ---\n",
      "Epoch : 45\n",
      "learning_rate: 0.000882903\n",
      "train_loss: 0.081508, valid_loss: 0.057104\n",
      "train_f1: 0.892944, valid_f1: 0.934254\n",
      "--- 28.680875539779663 seconds ---\n",
      "Epoch : 46\n",
      "learning_rate: 0.000875318\n",
      "train_loss: 0.081101, valid_loss: 0.056899\n",
      "train_f1: 0.894265, valid_f1: 0.935468\n",
      "--- 28.443907976150513 seconds ---\n",
      "Epoch : 47\n",
      "learning_rate: 0.000867529\n",
      "train_loss: 0.080720, valid_loss: 0.056763\n",
      "train_f1: 0.894588, valid_f1: 0.935424\n",
      "--- 29.421342849731445 seconds ---\n",
      "Epoch : 48\n",
      "learning_rate: 0.000859542\n",
      "train_loss: 0.079777, valid_loss: 0.056519\n",
      "train_f1: 0.896532, valid_f1: 0.936004\n",
      "--- 28.556822538375854 seconds ---\n",
      "Epoch : 49\n",
      "learning_rate: 0.000851359\n",
      "train_loss: 0.078612, valid_loss: 0.056458\n",
      "train_f1: 0.897042, valid_f1: 0.936549\n",
      "--- 28.532763957977295 seconds ---\n",
      "Epoch : 50\n",
      "learning_rate: 0.000842986\n",
      "train_loss: 0.078639, valid_loss: 0.056264\n",
      "train_f1: 0.897514, valid_f1: 0.936001\n",
      "--- 28.62248706817627 seconds ---\n",
      "Epoch : 51\n",
      "learning_rate: 0.000834428\n",
      "train_loss: 0.077724, valid_loss: 0.056280\n",
      "train_f1: 0.898723, valid_f1: 0.936077\n",
      "--- 28.372479915618896 seconds ---\n",
      "Epoch : 52\n",
      "learning_rate: 0.000825689\n",
      "train_loss: 0.077203, valid_loss: 0.055851\n",
      "train_f1: 0.898797, valid_f1: 0.935394\n",
      "--- 28.370578289031982 seconds ---\n",
      "Epoch : 53\n",
      "learning_rate: 0.000816773\n",
      "train_loss: 0.076621, valid_loss: 0.055605\n",
      "train_f1: 0.900386, valid_f1: 0.935921\n",
      "--- 28.38920521736145 seconds ---\n",
      "Epoch : 54\n",
      "learning_rate: 0.000807685\n",
      "train_loss: 0.076820, valid_loss: 0.055662\n",
      "train_f1: 0.900576, valid_f1: 0.936496\n",
      "--- 28.38017702102661 seconds ---\n",
      "Epoch : 55\n",
      "learning_rate: 0.000798431\n",
      "train_loss: 0.076123, valid_loss: 0.055230\n",
      "train_f1: 0.900939, valid_f1: 0.936251\n",
      "--- 28.232503175735474 seconds ---\n",
      "Epoch : 56\n",
      "learning_rate: 0.000789015\n",
      "train_loss: 0.075736, valid_loss: 0.055078\n",
      "train_f1: 0.902293, valid_f1: 0.936568\n",
      "--- 28.45737624168396 seconds ---\n",
      "Epoch : 57\n",
      "learning_rate: 0.000779443\n",
      "train_loss: 0.074520, valid_loss: 0.055565\n",
      "train_f1: 0.903189, valid_f1: 0.936518\n",
      "--- 28.399781942367554 seconds ---\n",
      "Epoch : 58\n",
      "learning_rate: 0.000769720\n",
      "train_loss: 0.074230, valid_loss: 0.055110\n",
      "train_f1: 0.902722, valid_f1: 0.934972\n",
      "--- 28.602325201034546 seconds ---\n",
      "Epoch : 59\n",
      "learning_rate: 0.000759850\n",
      "train_loss: 0.073682, valid_loss: 0.054735\n",
      "train_f1: 0.903988, valid_f1: 0.935945\n",
      "--- 28.336321115493774 seconds ---\n",
      "Epoch : 60\n",
      "learning_rate: 0.000749840\n",
      "train_loss: 0.073720, valid_loss: 0.054895\n",
      "train_f1: 0.904003, valid_f1: 0.934919\n",
      "--- 28.360114574432373 seconds ---\n",
      "Epoch : 61\n",
      "learning_rate: 0.000739695\n",
      "train_loss: 0.073352, valid_loss: 0.054561\n",
      "train_f1: 0.904667, valid_f1: 0.935842\n",
      "--- 28.599886178970337 seconds ---\n",
      "Epoch : 62\n",
      "learning_rate: 0.000729419\n",
      "train_loss: 0.072925, valid_loss: 0.054619\n",
      "train_f1: 0.905476, valid_f1: 0.936123\n",
      "--- 28.405267000198364 seconds ---\n",
      "Epoch : 63\n",
      "learning_rate: 0.000719020\n",
      "train_loss: 0.072426, valid_loss: 0.054174\n",
      "train_f1: 0.905684, valid_f1: 0.935972\n",
      "--- 28.52518367767334 seconds ---\n",
      "Epoch : 64\n",
      "learning_rate: 0.000708501\n",
      "train_loss: 0.072030, valid_loss: 0.054169\n",
      "train_f1: 0.906232, valid_f1: 0.936742\n",
      "--- 28.29146909713745 seconds ---\n",
      "Epoch : 65\n",
      "learning_rate: 0.000697870\n",
      "train_loss: 0.072023, valid_loss: 0.054417\n",
      "train_f1: 0.906439, valid_f1: 0.936594\n",
      "--- 28.78595232963562 seconds ---\n",
      "Epoch : 66\n",
      "learning_rate: 0.000687132\n",
      "train_loss: 0.071744, valid_loss: 0.054054\n",
      "train_f1: 0.906849, valid_f1: 0.936616\n",
      "--- 29.113343238830566 seconds ---\n",
      "Epoch : 67\n",
      "learning_rate: 0.000676292\n",
      "train_loss: 0.071355, valid_loss: 0.054218\n",
      "train_f1: 0.907257, valid_f1: 0.936538\n",
      "--- 28.651262283325195 seconds ---\n",
      "Epoch : 68\n",
      "learning_rate: 0.000665357\n",
      "train_loss: 0.071116, valid_loss: 0.053780\n",
      "train_f1: 0.907879, valid_f1: 0.936033\n",
      "--- 28.211432933807373 seconds ---\n",
      "Epoch : 69\n",
      "learning_rate: 0.000654333\n",
      "train_loss: 0.073243, valid_loss: 0.054236\n",
      "train_f1: 0.901599, valid_f1: 0.936380\n",
      "--- 27.950605392456055 seconds ---\n",
      "Epoch : 70\n",
      "learning_rate: 0.000643225\n",
      "train_loss: 0.070255, valid_loss: 0.053626\n",
      "train_f1: 0.908486, valid_f1: 0.936660\n",
      "--- 28.158833026885986 seconds ---\n",
      "Epoch : 71\n",
      "learning_rate: 0.000632039\n",
      "train_loss: 0.070351, valid_loss: 0.053897\n",
      "train_f1: 0.908373, valid_f1: 0.935813\n",
      "--- 28.198076248168945 seconds ---\n",
      "Epoch : 72\n",
      "learning_rate: 0.000620782\n",
      "train_loss: 0.069897, valid_loss: 0.053412\n",
      "train_f1: 0.909361, valid_f1: 0.936878\n",
      "--- 28.69951605796814 seconds ---\n",
      "Epoch : 73\n",
      "learning_rate: 0.000609459\n",
      "train_loss: 0.069208, valid_loss: 0.053514\n",
      "train_f1: 0.909403, valid_f1: 0.936584\n",
      "--- 28.479448795318604 seconds ---\n",
      "Epoch : 74\n",
      "learning_rate: 0.000598077\n",
      "train_loss: 0.069468, valid_loss: 0.053620\n",
      "train_f1: 0.909882, valid_f1: 0.936456\n",
      "--- 28.324886322021484 seconds ---\n",
      "Epoch : 75\n",
      "learning_rate: 0.000586642\n",
      "train_loss: 0.069219, valid_loss: 0.053127\n",
      "train_f1: 0.910170, valid_f1: 0.936753\n",
      "--- 29.039500951766968 seconds ---\n",
      "Epoch : 76\n",
      "learning_rate: 0.000575160\n",
      "train_loss: 0.068736, valid_loss: 0.053137\n",
      "train_f1: 0.910090, valid_f1: 0.936594\n",
      "--- 28.499928951263428 seconds ---\n",
      "Epoch : 77\n",
      "learning_rate: 0.000563638\n",
      "train_loss: 0.068373, valid_loss: 0.053695\n",
      "train_f1: 0.910490, valid_f1: 0.934106\n",
      "--- 28.27905583381653 seconds ---\n",
      "Epoch : 78\n",
      "learning_rate: 0.000552081\n",
      "train_loss: 0.068549, valid_loss: 0.053118\n",
      "train_f1: 0.910848, valid_f1: 0.936521\n",
      "--- 28.31850028038025 seconds ---\n",
      "Epoch : 79\n",
      "learning_rate: 0.000540495\n",
      "train_loss: 0.069001, valid_loss: 0.053021\n",
      "train_f1: 0.911249, valid_f1: 0.936783\n",
      "--- 28.38612651824951 seconds ---\n",
      "Epoch : 80\n",
      "learning_rate: 0.000528888\n",
      "train_loss: 0.067610, valid_loss: 0.052994\n",
      "train_f1: 0.911918, valid_f1: 0.936637\n",
      "--- 28.254600763320923 seconds ---\n",
      "Epoch : 81\n",
      "learning_rate: 0.000517265\n",
      "train_loss: 0.067750, valid_loss: 0.052825\n",
      "train_f1: 0.912147, valid_f1: 0.935668\n",
      "--- 28.535996675491333 seconds ---\n",
      "Epoch : 82\n",
      "learning_rate: 0.000505633\n",
      "train_loss: 0.067684, valid_loss: 0.052853\n",
      "train_f1: 0.912350, valid_f1: 0.936668\n",
      "--- 28.33510136604309 seconds ---\n",
      "Epoch : 83\n",
      "learning_rate: 0.000493998\n",
      "train_loss: 0.067085, valid_loss: 0.052816\n",
      "train_f1: 0.912645, valid_f1: 0.935396\n",
      "--- 28.45232915878296 seconds ---\n",
      "Epoch : 84\n",
      "learning_rate: 0.000482366\n",
      "train_loss: 0.066997, valid_loss: 0.052959\n",
      "train_f1: 0.912523, valid_f1: 0.936674\n",
      "--- 28.285924196243286 seconds ---\n",
      "Epoch : 85\n",
      "learning_rate: 0.000470743\n",
      "train_loss: 0.066970, valid_loss: 0.052892\n",
      "train_f1: 0.912998, valid_f1: 0.936317\n",
      "--- 28.344301223754883 seconds ---\n",
      "Epoch : 86\n",
      "learning_rate: 0.000459137\n",
      "train_loss: 0.067246, valid_loss: 0.052605\n",
      "train_f1: 0.912944, valid_f1: 0.936512\n",
      "--- 28.511553525924683 seconds ---\n",
      "Epoch : 87\n",
      "learning_rate: 0.000447552\n",
      "train_loss: 0.066146, valid_loss: 0.053224\n",
      "train_f1: 0.913284, valid_f1: 0.936071\n",
      "--- 28.58705997467041 seconds ---\n",
      "Epoch : 88\n",
      "learning_rate: 0.000435996\n",
      "train_loss: 0.066885, valid_loss: 0.052560\n",
      "train_f1: 0.912150, valid_f1: 0.936680\n",
      "--- 28.605608224868774 seconds ---\n",
      "Epoch : 89\n",
      "learning_rate: 0.000424475\n",
      "train_loss: 0.066215, valid_loss: 0.052855\n",
      "train_f1: 0.913596, valid_f1: 0.936683\n",
      "--- 28.523226499557495 seconds ---\n",
      "Epoch : 90\n",
      "learning_rate: 0.000412994\n",
      "train_loss: 0.065885, valid_loss: 0.052621\n",
      "train_f1: 0.914487, valid_f1: 0.936573\n",
      "--- 28.393223762512207 seconds ---\n",
      "Epoch : 91\n",
      "learning_rate: 0.000401561\n",
      "train_loss: 0.065750, valid_loss: 0.052569\n",
      "train_f1: 0.914185, valid_f1: 0.936530\n",
      "--- 28.407665252685547 seconds ---\n",
      "Epoch : 92\n",
      "learning_rate: 0.000390181\n",
      "train_loss: 0.066089, valid_loss: 0.052762\n",
      "train_f1: 0.914093, valid_f1: 0.936074\n",
      "Early Stopping...\n",
      "Best Val Score: 0.936878\n",
      "Fold : 1\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "train_loss: 2.015845, valid_loss: 1.900992\n",
      "train_f1: 0.042716, valid_f1: 0.006793\n",
      "--- 28.458447694778442 seconds ---\n",
      "Epoch : 1\n",
      "learning_rate: 0.000011938\n",
      "train_loss: 1.871899, valid_loss: 1.646720\n",
      "train_f1: 0.046611, valid_f1: 0.023725\n",
      "--- 28.46841311454773 seconds ---\n",
      "Epoch : 2\n",
      "learning_rate: 0.000044274\n",
      "train_loss: 1.545115, valid_loss: 1.130016\n",
      "train_f1: 0.070545, valid_f1: 0.073310\n",
      "--- 28.615108728408813 seconds ---\n",
      "Epoch : 3\n",
      "learning_rate: 0.000096592\n",
      "train_loss: 1.148351, valid_loss: 0.768558\n",
      "train_f1: 0.108449, valid_f1: 0.119708\n",
      "--- 28.62209987640381 seconds ---\n",
      "Epoch : 4\n",
      "learning_rate: 0.000166599\n",
      "train_loss: 0.905984, valid_loss: 0.631076\n",
      "train_f1: 0.160771, valid_f1: 0.169754\n",
      "--- 28.532881259918213 seconds ---\n",
      "Epoch : 5\n",
      "learning_rate: 0.000251230\n",
      "train_loss: 0.743878, valid_loss: 0.494313\n",
      "train_f1: 0.229760, valid_f1: 0.330386\n",
      "--- 28.624594688415527 seconds ---\n",
      "Epoch : 6\n",
      "learning_rate: 0.000346779\n",
      "train_loss: 0.607921, valid_loss: 0.389563\n",
      "train_f1: 0.308502, valid_f1: 0.424183\n",
      "--- 28.368006944656372 seconds ---\n",
      "Epoch : 7\n",
      "learning_rate: 0.000449060\n",
      "train_loss: 0.503542, valid_loss: 0.322912\n",
      "train_f1: 0.373808, valid_f1: 0.472654\n",
      "--- 28.513891220092773 seconds ---\n",
      "Epoch : 8\n",
      "learning_rate: 0.000553594\n",
      "train_loss: 0.426457, valid_loss: 0.260718\n",
      "train_f1: 0.435412, valid_f1: 0.575235\n",
      "--- 27.93931818008423 seconds ---\n",
      "Epoch : 9\n",
      "learning_rate: 0.000655802\n",
      "train_loss: 0.349010, valid_loss: 0.188173\n",
      "train_f1: 0.506043, valid_f1: 0.600672\n",
      "--- 28.281318187713623 seconds ---\n",
      "Epoch : 10\n",
      "learning_rate: 0.000751209\n",
      "train_loss: 0.280728, valid_loss: 0.150015\n",
      "train_f1: 0.579840, valid_f1: 0.761325\n",
      "--- 28.22545027732849 seconds ---\n",
      "Epoch : 11\n",
      "learning_rate: 0.000835636\n",
      "train_loss: 0.234551, valid_loss: 0.119261\n",
      "train_f1: 0.637042, valid_f1: 0.800889\n",
      "--- 28.115573406219482 seconds ---\n",
      "Epoch : 12\n",
      "learning_rate: 0.000905384\n",
      "train_loss: 0.199094, valid_loss: 0.096761\n",
      "train_f1: 0.687959, valid_f1: 0.807077\n",
      "--- 28.298073768615723 seconds ---\n",
      "Epoch : 13\n",
      "learning_rate: 0.000957400\n",
      "train_loss: 0.171798, valid_loss: 0.085187\n",
      "train_f1: 0.725888, valid_f1: 0.826766\n",
      "--- 28.24826169013977 seconds ---\n",
      "Epoch : 14\n",
      "learning_rate: 0.000989405\n",
      "train_loss: 0.153220, valid_loss: 0.078231\n",
      "train_f1: 0.755061, valid_f1: 0.834080\n",
      "--- 28.21784281730652 seconds ---\n",
      "Epoch : 15\n",
      "learning_rate: 0.001000000\n",
      "train_loss: 0.140699, valid_loss: 0.073932\n",
      "train_f1: 0.775057, valid_f1: 0.840612\n",
      "--- 28.495548248291016 seconds ---\n",
      "Epoch : 16\n",
      "learning_rate: 0.000999860\n",
      "train_loss: 0.131132, valid_loss: 0.071179\n",
      "train_f1: 0.788766, valid_f1: 0.842406\n",
      "--- 28.502161502838135 seconds ---\n",
      "Epoch : 17\n",
      "learning_rate: 0.000999450\n",
      "train_loss: 0.123129, valid_loss: 0.069601\n",
      "train_f1: 0.799964, valid_f1: 0.844567\n",
      "--- 28.403334379196167 seconds ---\n",
      "Epoch : 18\n",
      "learning_rate: 0.000998769\n",
      "train_loss: 0.118505, valid_loss: 0.067499\n",
      "train_f1: 0.807785, valid_f1: 0.845521\n",
      "--- 28.517537355422974 seconds ---\n",
      "Epoch : 19\n",
      "learning_rate: 0.000997818\n",
      "train_loss: 0.113383, valid_loss: 0.066163\n",
      "train_f1: 0.813793, valid_f1: 0.846047\n",
      "--- 28.165976524353027 seconds ---\n",
      "Epoch : 20\n",
      "learning_rate: 0.000996598\n",
      "train_loss: 0.109231, valid_loss: 0.064838\n",
      "train_f1: 0.819886, valid_f1: 0.846228\n",
      "--- 28.376839637756348 seconds ---\n",
      "Epoch : 21\n",
      "learning_rate: 0.000995108\n",
      "train_loss: 0.106671, valid_loss: 0.063544\n",
      "train_f1: 0.824500, valid_f1: 0.846589\n",
      "--- 28.563647270202637 seconds ---\n",
      "Epoch : 22\n",
      "learning_rate: 0.000993351\n",
      "train_loss: 0.103017, valid_loss: 0.063166\n",
      "train_f1: 0.829780, valid_f1: 0.846482\n",
      "--- 28.514716386795044 seconds ---\n",
      "Epoch : 23\n",
      "learning_rate: 0.000991326\n",
      "train_loss: 0.101439, valid_loss: 0.061782\n",
      "train_f1: 0.835261, valid_f1: 0.847090\n",
      "--- 28.38259792327881 seconds ---\n",
      "Epoch : 24\n",
      "learning_rate: 0.000989035\n",
      "train_loss: 0.099095, valid_loss: 0.060737\n",
      "train_f1: 0.839433, valid_f1: 0.847954\n",
      "--- 28.229554891586304 seconds ---\n",
      "Epoch : 25\n",
      "learning_rate: 0.000986480\n",
      "train_loss: 0.097571, valid_loss: 0.059863\n",
      "train_f1: 0.844926, valid_f1: 0.847709\n",
      "--- 28.16742730140686 seconds ---\n",
      "Epoch : 26\n",
      "learning_rate: 0.000983661\n",
      "train_loss: 0.095837, valid_loss: 0.059514\n",
      "train_f1: 0.850252, valid_f1: 0.860810\n",
      "--- 28.255079746246338 seconds ---\n",
      "Epoch : 27\n",
      "learning_rate: 0.000980580\n",
      "train_loss: 0.093603, valid_loss: 0.058250\n",
      "train_f1: 0.855513, valid_f1: 0.869217\n",
      "--- 28.33810043334961 seconds ---\n",
      "Epoch : 28\n",
      "learning_rate: 0.000977239\n",
      "train_loss: 0.092414, valid_loss: 0.057961\n",
      "train_f1: 0.859986, valid_f1: 0.901610\n",
      "--- 28.2523136138916 seconds ---\n",
      "Epoch : 29\n",
      "learning_rate: 0.000973639\n",
      "train_loss: 0.094582, valid_loss: 0.058187\n",
      "train_f1: 0.856234, valid_f1: 0.890757\n",
      "--- 28.211004972457886 seconds ---\n",
      "Epoch : 30\n",
      "learning_rate: 0.000969783\n",
      "train_loss: 0.089647, valid_loss: 0.056976\n",
      "train_f1: 0.865561, valid_f1: 0.922432\n",
      "--- 28.280874729156494 seconds ---\n",
      "Epoch : 31\n",
      "learning_rate: 0.000965673\n",
      "train_loss: 0.089321, valid_loss: 0.056232\n",
      "train_f1: 0.868427, valid_f1: 0.920619\n",
      "--- 28.322614192962646 seconds ---\n",
      "Epoch : 32\n",
      "learning_rate: 0.000961310\n",
      "train_loss: 0.087027, valid_loss: 0.055684\n",
      "train_f1: 0.874601, valid_f1: 0.934168\n",
      "--- 28.60919427871704 seconds ---\n",
      "Epoch : 33\n",
      "learning_rate: 0.000956698\n",
      "train_loss: 0.086553, valid_loss: 0.054648\n",
      "train_f1: 0.877641, valid_f1: 0.931173\n",
      "--- 28.371991872787476 seconds ---\n",
      "Epoch : 34\n",
      "learning_rate: 0.000951838\n",
      "train_loss: 0.085482, valid_loss: 0.054696\n",
      "train_f1: 0.880066, valid_f1: 0.931877\n",
      "--- 28.359733819961548 seconds ---\n",
      "Epoch : 35\n",
      "learning_rate: 0.000946733\n",
      "train_loss: 0.084563, valid_loss: 0.054381\n",
      "train_f1: 0.883820, valid_f1: 0.930965\n",
      "--- 28.60281276702881 seconds ---\n",
      "Epoch : 36\n",
      "learning_rate: 0.000941387\n",
      "train_loss: 0.083519, valid_loss: 0.053913\n",
      "train_f1: 0.885557, valid_f1: 0.936422\n",
      "--- 28.47335910797119 seconds ---\n",
      "Epoch : 37\n",
      "learning_rate: 0.000935802\n",
      "train_loss: 0.082706, valid_loss: 0.053974\n",
      "train_f1: 0.888113, valid_f1: 0.934046\n",
      "--- 28.391122817993164 seconds ---\n",
      "Epoch : 38\n",
      "learning_rate: 0.000929980\n",
      "train_loss: 0.082105, valid_loss: 0.052811\n",
      "train_f1: 0.889400, valid_f1: 0.933766\n",
      "--- 28.40698504447937 seconds ---\n",
      "Epoch : 39\n",
      "learning_rate: 0.000923926\n",
      "train_loss: 0.081445, valid_loss: 0.052913\n",
      "train_f1: 0.891346, valid_f1: 0.934712\n",
      "--- 28.397541046142578 seconds ---\n",
      "Epoch : 40\n",
      "learning_rate: 0.000917642\n",
      "train_loss: 0.080665, valid_loss: 0.052404\n",
      "train_f1: 0.893953, valid_f1: 0.937152\n",
      "--- 28.36972737312317 seconds ---\n",
      "Epoch : 41\n",
      "learning_rate: 0.000911132\n",
      "train_loss: 0.079615, valid_loss: 0.051994\n",
      "train_f1: 0.895268, valid_f1: 0.935391\n",
      "--- 28.47194790840149 seconds ---\n",
      "Epoch : 42\n",
      "learning_rate: 0.000904400\n",
      "train_loss: 0.078520, valid_loss: 0.052821\n",
      "train_f1: 0.896487, valid_f1: 0.933035\n",
      "--- 28.419201374053955 seconds ---\n",
      "Epoch : 43\n",
      "learning_rate: 0.000897448\n",
      "train_loss: 0.078663, valid_loss: 0.051700\n",
      "train_f1: 0.897548, valid_f1: 0.937506\n",
      "--- 28.397323846817017 seconds ---\n",
      "Epoch : 44\n",
      "learning_rate: 0.000890282\n",
      "train_loss: 0.077552, valid_loss: 0.051282\n",
      "train_f1: 0.898780, valid_f1: 0.936361\n",
      "--- 28.417052507400513 seconds ---\n",
      "Epoch : 45\n",
      "learning_rate: 0.000882903\n",
      "train_loss: 0.077158, valid_loss: 0.050919\n",
      "train_f1: 0.899786, valid_f1: 0.937120\n",
      "--- 28.38825273513794 seconds ---\n",
      "Epoch : 46\n",
      "learning_rate: 0.000875318\n",
      "train_loss: 0.076493, valid_loss: 0.050814\n",
      "train_f1: 0.901250, valid_f1: 0.937857\n",
      "--- 28.460506677627563 seconds ---\n",
      "Epoch : 47\n",
      "learning_rate: 0.000867529\n",
      "train_loss: 0.076429, valid_loss: 0.050883\n",
      "train_f1: 0.901641, valid_f1: 0.935557\n",
      "--- 28.418296098709106 seconds ---\n",
      "Epoch : 48\n",
      "learning_rate: 0.000859542\n",
      "train_loss: 0.075319, valid_loss: 0.050494\n",
      "train_f1: 0.902732, valid_f1: 0.937056\n",
      "--- 28.61823582649231 seconds ---\n",
      "Epoch : 49\n",
      "learning_rate: 0.000851359\n",
      "train_loss: 0.075023, valid_loss: 0.050224\n",
      "train_f1: 0.903596, valid_f1: 0.935751\n",
      "--- 28.329885244369507 seconds ---\n",
      "Epoch : 50\n",
      "learning_rate: 0.000842986\n",
      "train_loss: 0.075146, valid_loss: 0.049819\n",
      "train_f1: 0.904598, valid_f1: 0.938152\n",
      "--- 28.37868595123291 seconds ---\n",
      "Epoch : 51\n",
      "learning_rate: 0.000834428\n",
      "train_loss: 0.073676, valid_loss: 0.049942\n",
      "train_f1: 0.905237, valid_f1: 0.935869\n",
      "--- 28.471793174743652 seconds ---\n",
      "Epoch : 52\n",
      "learning_rate: 0.000825689\n",
      "train_loss: 0.073753, valid_loss: 0.049996\n",
      "train_f1: 0.906033, valid_f1: 0.937552\n",
      "--- 28.4175021648407 seconds ---\n",
      "Epoch : 53\n",
      "learning_rate: 0.000816773\n",
      "train_loss: 0.073425, valid_loss: 0.049702\n",
      "train_f1: 0.906284, valid_f1: 0.937069\n",
      "--- 28.270023345947266 seconds ---\n",
      "Epoch : 54\n",
      "learning_rate: 0.000807685\n",
      "train_loss: 0.072852, valid_loss: 0.049543\n",
      "train_f1: 0.906787, valid_f1: 0.937671\n",
      "--- 28.30426859855652 seconds ---\n",
      "Epoch : 55\n",
      "learning_rate: 0.000798431\n",
      "train_loss: 0.072143, valid_loss: 0.049692\n",
      "train_f1: 0.907921, valid_f1: 0.938044\n",
      "--- 28.364603281021118 seconds ---\n",
      "Epoch : 56\n",
      "learning_rate: 0.000789015\n",
      "train_loss: 0.072113, valid_loss: 0.049151\n",
      "train_f1: 0.908100, valid_f1: 0.938189\n",
      "--- 28.294573068618774 seconds ---\n",
      "Epoch : 57\n",
      "learning_rate: 0.000779443\n",
      "train_loss: 0.071969, valid_loss: 0.049082\n",
      "train_f1: 0.909325, valid_f1: 0.937364\n",
      "--- 28.309126615524292 seconds ---\n",
      "Epoch : 58\n",
      "learning_rate: 0.000769720\n",
      "train_loss: 0.071132, valid_loss: 0.048983\n",
      "train_f1: 0.909678, valid_f1: 0.936083\n",
      "--- 28.466323137283325 seconds ---\n",
      "Epoch : 59\n",
      "learning_rate: 0.000759850\n",
      "train_loss: 0.071103, valid_loss: 0.048864\n",
      "train_f1: 0.910003, valid_f1: 0.937992\n",
      "--- 28.398346185684204 seconds ---\n",
      "Epoch : 60\n",
      "learning_rate: 0.000749840\n",
      "train_loss: 0.070289, valid_loss: 0.048746\n",
      "train_f1: 0.910142, valid_f1: 0.937899\n",
      "--- 28.42307949066162 seconds ---\n",
      "Epoch : 61\n",
      "learning_rate: 0.000739695\n",
      "train_loss: 0.070255, valid_loss: 0.048476\n",
      "train_f1: 0.911045, valid_f1: 0.938034\n",
      "--- 28.730536937713623 seconds ---\n",
      "Epoch : 62\n",
      "learning_rate: 0.000729419\n",
      "train_loss: 0.069831, valid_loss: 0.048391\n",
      "train_f1: 0.911513, valid_f1: 0.938326\n",
      "--- 28.129714727401733 seconds ---\n",
      "Epoch : 63\n",
      "learning_rate: 0.000719020\n",
      "train_loss: 0.069785, valid_loss: 0.048719\n",
      "train_f1: 0.911812, valid_f1: 0.937762\n",
      "--- 28.203176736831665 seconds ---\n",
      "Epoch : 64\n",
      "learning_rate: 0.000708501\n",
      "train_loss: 0.069916, valid_loss: 0.048557\n",
      "train_f1: 0.911970, valid_f1: 0.938063\n",
      "--- 28.076336145401 seconds ---\n",
      "Epoch : 65\n",
      "learning_rate: 0.000697870\n",
      "train_loss: 0.068751, valid_loss: 0.048660\n",
      "train_f1: 0.912880, valid_f1: 0.938203\n",
      "--- 28.206613302230835 seconds ---\n",
      "Epoch : 66\n",
      "learning_rate: 0.000687132\n",
      "train_loss: 0.068572, valid_loss: 0.048147\n",
      "train_f1: 0.912971, valid_f1: 0.938100\n",
      "--- 28.20445466041565 seconds ---\n",
      "Epoch : 67\n",
      "learning_rate: 0.000676292\n",
      "train_loss: 0.068519, valid_loss: 0.048128\n",
      "train_f1: 0.913868, valid_f1: 0.937802\n",
      "--- 28.111112356185913 seconds ---\n",
      "Epoch : 68\n",
      "learning_rate: 0.000665357\n",
      "train_loss: 0.068385, valid_loss: 0.048251\n",
      "train_f1: 0.913817, valid_f1: 0.938376\n",
      "--- 28.3821017742157 seconds ---\n",
      "Epoch : 69\n",
      "learning_rate: 0.000654333\n",
      "train_loss: 0.067708, valid_loss: 0.048225\n",
      "train_f1: 0.914252, valid_f1: 0.937109\n",
      "--- 28.425459623336792 seconds ---\n",
      "Epoch : 70\n",
      "learning_rate: 0.000643225\n",
      "train_loss: 0.067943, valid_loss: 0.048320\n",
      "train_f1: 0.914032, valid_f1: 0.937411\n",
      "--- 28.4218590259552 seconds ---\n",
      "Epoch : 71\n",
      "learning_rate: 0.000632039\n",
      "train_loss: 0.067474, valid_loss: 0.048072\n",
      "train_f1: 0.914190, valid_f1: 0.938637\n",
      "--- 28.412360191345215 seconds ---\n",
      "Epoch : 72\n",
      "learning_rate: 0.000620782\n",
      "train_loss: 0.067516, valid_loss: 0.048025\n",
      "train_f1: 0.914670, valid_f1: 0.938743\n",
      "--- 28.4274742603302 seconds ---\n",
      "Epoch : 73\n",
      "learning_rate: 0.000609459\n",
      "train_loss: 0.067078, valid_loss: 0.047776\n",
      "train_f1: 0.915169, valid_f1: 0.938844\n",
      "--- 28.429515600204468 seconds ---\n",
      "Epoch : 74\n",
      "learning_rate: 0.000598077\n",
      "train_loss: 0.077011, valid_loss: 0.048521\n",
      "train_f1: 0.899958, valid_f1: 0.938217\n",
      "--- 28.407025814056396 seconds ---\n",
      "Epoch : 75\n",
      "learning_rate: 0.000586642\n",
      "train_loss: 0.067789, valid_loss: 0.048127\n",
      "train_f1: 0.914526, valid_f1: 0.938787\n",
      "--- 28.40418553352356 seconds ---\n",
      "Epoch : 76\n",
      "learning_rate: 0.000575160\n",
      "train_loss: 0.066846, valid_loss: 0.048006\n",
      "train_f1: 0.915522, valid_f1: 0.937922\n",
      "--- 27.998921632766724 seconds ---\n",
      "Epoch : 77\n",
      "learning_rate: 0.000563638\n",
      "train_loss: 0.066691, valid_loss: 0.047595\n",
      "train_f1: 0.915969, valid_f1: 0.938734\n",
      "--- 27.967602968215942 seconds ---\n",
      "Epoch : 78\n",
      "learning_rate: 0.000552081\n",
      "train_loss: 0.066087, valid_loss: 0.047828\n",
      "train_f1: 0.916387, valid_f1: 0.938500\n",
      "--- 28.45113706588745 seconds ---\n",
      "Epoch : 79\n",
      "learning_rate: 0.000540495\n",
      "train_loss: 0.065621, valid_loss: 0.047607\n",
      "train_f1: 0.916808, valid_f1: 0.938192\n",
      "--- 28.1028048992157 seconds ---\n",
      "Epoch : 80\n",
      "learning_rate: 0.000528888\n",
      "train_loss: 0.065561, valid_loss: 0.047620\n",
      "train_f1: 0.916702, valid_f1: 0.938909\n",
      "--- 28.44163203239441 seconds ---\n",
      "Epoch : 81\n",
      "learning_rate: 0.000517265\n",
      "train_loss: 0.065041, valid_loss: 0.047482\n",
      "train_f1: 0.917106, valid_f1: 0.938508\n",
      "--- 28.289692878723145 seconds ---\n",
      "Epoch : 82\n",
      "learning_rate: 0.000505633\n",
      "train_loss: 0.065388, valid_loss: 0.047532\n",
      "train_f1: 0.917223, valid_f1: 0.938894\n",
      "--- 28.45457100868225 seconds ---\n",
      "Epoch : 83\n",
      "learning_rate: 0.000493998\n",
      "train_loss: 0.065124, valid_loss: 0.047803\n",
      "train_f1: 0.916938, valid_f1: 0.938521\n",
      "--- 28.407259464263916 seconds ---\n",
      "Epoch : 84\n",
      "learning_rate: 0.000482366\n",
      "train_loss: 0.064835, valid_loss: 0.047637\n",
      "train_f1: 0.917539, valid_f1: 0.937269\n",
      "--- 28.31345844268799 seconds ---\n",
      "Epoch : 85\n",
      "learning_rate: 0.000470743\n",
      "train_loss: 0.064625, valid_loss: 0.047516\n",
      "train_f1: 0.917581, valid_f1: 0.938831\n",
      "--- 28.306668281555176 seconds ---\n",
      "Epoch : 86\n",
      "learning_rate: 0.000459137\n",
      "train_loss: 0.064715, valid_loss: 0.047356\n",
      "train_f1: 0.917346, valid_f1: 0.938980\n",
      "--- 28.339675664901733 seconds ---\n",
      "Epoch : 87\n",
      "learning_rate: 0.000447552\n",
      "train_loss: 0.064391, valid_loss: 0.047347\n",
      "train_f1: 0.917758, valid_f1: 0.938692\n",
      "--- 28.37244725227356 seconds ---\n",
      "Epoch : 88\n",
      "learning_rate: 0.000435996\n",
      "train_loss: 0.064450, valid_loss: 0.047440\n",
      "train_f1: 0.918077, valid_f1: 0.938857\n",
      "--- 28.65717363357544 seconds ---\n",
      "Epoch : 89\n",
      "learning_rate: 0.000424475\n",
      "train_loss: 0.064649, valid_loss: 0.047283\n",
      "train_f1: 0.917817, valid_f1: 0.938920\n",
      "--- 28.425981044769287 seconds ---\n",
      "Epoch : 90\n",
      "learning_rate: 0.000412994\n",
      "train_loss: 0.064334, valid_loss: 0.047299\n",
      "train_f1: 0.918423, valid_f1: 0.939124\n",
      "--- 28.444106101989746 seconds ---\n",
      "Epoch : 91\n",
      "learning_rate: 0.000401561\n",
      "train_loss: 0.063718, valid_loss: 0.047323\n",
      "train_f1: 0.918410, valid_f1: 0.939044\n",
      "--- 28.38163661956787 seconds ---\n",
      "Epoch : 92\n",
      "learning_rate: 0.000390181\n",
      "train_loss: 0.063731, valid_loss: 0.047240\n",
      "train_f1: 0.918408, valid_f1: 0.938863\n",
      "--- 28.43855619430542 seconds ---\n",
      "Epoch : 93\n",
      "learning_rate: 0.000378860\n",
      "train_loss: 0.063533, valid_loss: 0.047197\n",
      "train_f1: 0.918989, valid_f1: 0.938957\n",
      "--- 28.374151945114136 seconds ---\n",
      "Epoch : 94\n",
      "learning_rate: 0.000367605\n",
      "train_loss: 0.064097, valid_loss: 0.047156\n",
      "train_f1: 0.918332, valid_f1: 0.939007\n",
      "--- 28.46549654006958 seconds ---\n",
      "Epoch : 95\n",
      "learning_rate: 0.000356422\n",
      "train_loss: 0.063675, valid_loss: 0.047179\n",
      "train_f1: 0.919377, valid_f1: 0.938802\n",
      "--- 28.433077096939087 seconds ---\n",
      "Epoch : 96\n",
      "learning_rate: 0.000345316\n",
      "train_loss: 0.063472, valid_loss: 0.047018\n",
      "train_f1: 0.918755, valid_f1: 0.939110\n",
      "--- 28.611579418182373 seconds ---\n",
      "Epoch : 97\n",
      "learning_rate: 0.000334294\n",
      "train_loss: 0.063497, valid_loss: 0.047211\n",
      "train_f1: 0.918958, valid_f1: 0.939223\n",
      "--- 28.38087773323059 seconds ---\n",
      "Epoch : 98\n",
      "learning_rate: 0.000323362\n",
      "train_loss: 0.063120, valid_loss: 0.047149\n",
      "train_f1: 0.919188, valid_f1: 0.939322\n",
      "--- 28.426632404327393 seconds ---\n",
      "Epoch : 99\n",
      "learning_rate: 0.000312526\n",
      "train_loss: 0.063088, valid_loss: 0.047135\n",
      "train_f1: 0.918732, valid_f1: 0.939332\n",
      "--- 28.427102088928223 seconds ---\n",
      "Epoch : 100\n",
      "learning_rate: 0.000301791\n",
      "train_loss: 0.063562, valid_loss: 0.047027\n",
      "train_f1: 0.919609, valid_f1: 0.939056\n",
      "--- 28.39516019821167 seconds ---\n",
      "Epoch : 101\n",
      "learning_rate: 0.000291163\n",
      "train_loss: 0.063108, valid_loss: 0.046985\n",
      "train_f1: 0.919419, valid_f1: 0.939295\n",
      "--- 28.367267608642578 seconds ---\n",
      "Epoch : 102\n",
      "learning_rate: 0.000280649\n",
      "train_loss: 0.063497, valid_loss: 0.047140\n",
      "train_f1: 0.919885, valid_f1: 0.938852\n",
      "--- 28.65067434310913 seconds ---\n",
      "Epoch : 103\n",
      "learning_rate: 0.000270253\n",
      "train_loss: 0.063466, valid_loss: 0.046983\n",
      "train_f1: 0.919748, valid_f1: 0.939027\n",
      "--- 28.355972051620483 seconds ---\n",
      "Epoch : 104\n",
      "learning_rate: 0.000259981\n",
      "train_loss: 0.062401, valid_loss: 0.046982\n",
      "train_f1: 0.919874, valid_f1: 0.939088\n",
      "--- 28.496780395507812 seconds ---\n",
      "Epoch : 105\n",
      "learning_rate: 0.000249840\n",
      "train_loss: 0.063097, valid_loss: 0.046922\n",
      "train_f1: 0.919634, valid_f1: 0.939380\n",
      "--- 28.713654041290283 seconds ---\n",
      "Epoch : 106\n",
      "learning_rate: 0.000239834\n",
      "train_loss: 0.062671, valid_loss: 0.046897\n",
      "train_f1: 0.919928, valid_f1: 0.939398\n",
      "--- 28.4451642036438 seconds ---\n",
      "Epoch : 107\n",
      "learning_rate: 0.000229969\n",
      "train_loss: 0.062453, valid_loss: 0.046872\n",
      "train_f1: 0.919862, valid_f1: 0.939468\n",
      "--- 28.49878239631653 seconds ---\n",
      "Epoch : 108\n",
      "learning_rate: 0.000220251\n",
      "train_loss: 0.062369, valid_loss: 0.046897\n",
      "train_f1: 0.920243, valid_f1: 0.938792\n",
      "--- 28.418339252471924 seconds ---\n",
      "Epoch : 109\n",
      "learning_rate: 0.000210683\n",
      "train_loss: 0.062918, valid_loss: 0.046897\n",
      "train_f1: 0.920295, valid_f1: 0.939086\n",
      "--- 28.500296354293823 seconds ---\n",
      "Epoch : 110\n",
      "learning_rate: 0.000201273\n",
      "train_loss: 0.062197, valid_loss: 0.046773\n",
      "train_f1: 0.920448, valid_f1: 0.939375\n",
      "--- 28.42473530769348 seconds ---\n",
      "Epoch : 111\n",
      "learning_rate: 0.000192024\n",
      "train_loss: 0.062391, valid_loss: 0.046884\n",
      "train_f1: 0.920337, valid_f1: 0.939197\n",
      "--- 28.3669536113739 seconds ---\n",
      "Epoch : 112\n",
      "learning_rate: 0.000182942\n",
      "train_loss: 0.061924, valid_loss: 0.046850\n",
      "train_f1: 0.920139, valid_f1: 0.939326\n",
      "--- 28.62706422805786 seconds ---\n",
      "Epoch : 113\n",
      "learning_rate: 0.000174031\n",
      "train_loss: 0.061983, valid_loss: 0.046926\n",
      "train_f1: 0.920252, valid_f1: 0.939138\n",
      "--- 28.404592037200928 seconds ---\n",
      "Epoch : 114\n",
      "learning_rate: 0.000165298\n",
      "train_loss: 0.061723, valid_loss: 0.046833\n",
      "train_f1: 0.920685, valid_f1: 0.939378\n",
      "--- 28.838321685791016 seconds ---\n",
      "Epoch : 115\n",
      "learning_rate: 0.000156745\n",
      "train_loss: 0.061957, valid_loss: 0.046780\n",
      "train_f1: 0.920711, valid_f1: 0.939354\n",
      "--- 29.696476221084595 seconds ---\n",
      "Epoch : 116\n",
      "learning_rate: 0.000148378\n",
      "train_loss: 0.062122, valid_loss: 0.046733\n",
      "train_f1: 0.920734, valid_f1: 0.939131\n",
      "--- 29.353605031967163 seconds ---\n",
      "Epoch : 117\n",
      "learning_rate: 0.000140202\n",
      "train_loss: 0.061902, valid_loss: 0.046922\n",
      "train_f1: 0.920839, valid_f1: 0.938621\n",
      "--- 28.76340341567993 seconds ---\n",
      "Epoch : 118\n",
      "learning_rate: 0.000132220\n",
      "train_loss: 0.061599, valid_loss: 0.046751\n",
      "train_f1: 0.920486, valid_f1: 0.939358\n",
      "--- 28.41864776611328 seconds ---\n",
      "Epoch : 119\n",
      "learning_rate: 0.000124438\n",
      "train_loss: 0.061396, valid_loss: 0.046784\n",
      "train_f1: 0.921031, valid_f1: 0.939266\n",
      "--- 28.298187255859375 seconds ---\n",
      "Epoch : 120\n",
      "learning_rate: 0.000116859\n",
      "train_loss: 0.061762, valid_loss: 0.046715\n",
      "train_f1: 0.921033, valid_f1: 0.939226\n",
      "--- 28.62472152709961 seconds ---\n",
      "Epoch : 121\n",
      "learning_rate: 0.000109488\n",
      "train_loss: 0.061482, valid_loss: 0.046714\n",
      "train_f1: 0.920626, valid_f1: 0.939443\n",
      "--- 28.49261164665222 seconds ---\n",
      "Epoch : 122\n",
      "learning_rate: 0.000102328\n",
      "train_loss: 0.061731, valid_loss: 0.046723\n",
      "train_f1: 0.920908, valid_f1: 0.939313\n",
      "--- 28.415568113327026 seconds ---\n",
      "Epoch : 123\n",
      "learning_rate: 0.000095383\n",
      "train_loss: 0.061704, valid_loss: 0.046732\n",
      "train_f1: 0.920967, valid_f1: 0.939381\n",
      "--- 28.56149125099182 seconds ---\n",
      "Epoch : 124\n",
      "learning_rate: 0.000088658\n",
      "train_loss: 0.061912, valid_loss: 0.046829\n",
      "train_f1: 0.920924, valid_f1: 0.939373\n",
      "--- 28.56588077545166 seconds ---\n",
      "Epoch : 125\n",
      "learning_rate: 0.000082155\n",
      "train_loss: 0.061473, valid_loss: 0.046683\n",
      "train_f1: 0.921057, valid_f1: 0.939606\n",
      "--- 28.246376991271973 seconds ---\n",
      "Epoch : 126\n",
      "learning_rate: 0.000075878\n",
      "train_loss: 0.061569, valid_loss: 0.046704\n",
      "train_f1: 0.921279, valid_f1: 0.939527\n",
      "--- 28.329649925231934 seconds ---\n",
      "Epoch : 127\n",
      "learning_rate: 0.000069831\n",
      "train_loss: 0.061051, valid_loss: 0.046736\n",
      "train_f1: 0.921244, valid_f1: 0.939410\n",
      "--- 28.37226104736328 seconds ---\n",
      "Epoch : 128\n",
      "learning_rate: 0.000064017\n",
      "train_loss: 0.061780, valid_loss: 0.046736\n",
      "train_f1: 0.921057, valid_f1: 0.939317\n",
      "--- 28.2742760181427 seconds ---\n",
      "Epoch : 129\n",
      "learning_rate: 0.000058440\n",
      "train_loss: 0.061108, valid_loss: 0.046756\n",
      "train_f1: 0.921522, valid_f1: 0.939494\n",
      "--- 28.284743070602417 seconds ---\n",
      "Epoch : 130\n",
      "learning_rate: 0.000053101\n",
      "train_loss: 0.061557, valid_loss: 0.046701\n",
      "train_f1: 0.921501, valid_f1: 0.939530\n",
      "--- 28.452070713043213 seconds ---\n",
      "Epoch : 131\n",
      "learning_rate: 0.000048004\n",
      "train_loss: 0.061554, valid_loss: 0.046737\n",
      "train_f1: 0.921476, valid_f1: 0.939547\n",
      "--- 28.40637516975403 seconds ---\n",
      "Epoch : 132\n",
      "learning_rate: 0.000043152\n",
      "train_loss: 0.061062, valid_loss: 0.046718\n",
      "train_f1: 0.921597, valid_f1: 0.939426\n",
      "--- 28.408889293670654 seconds ---\n",
      "Epoch : 133\n",
      "learning_rate: 0.000038548\n",
      "train_loss: 0.061114, valid_loss: 0.046734\n",
      "train_f1: 0.921179, valid_f1: 0.939448\n",
      "--- 28.391828060150146 seconds ---\n",
      "Epoch : 134\n",
      "learning_rate: 0.000034193\n",
      "train_loss: 0.061181, valid_loss: 0.046679\n",
      "train_f1: 0.921360, valid_f1: 0.939473\n",
      "--- 28.35832643508911 seconds ---\n",
      "Epoch : 135\n",
      "learning_rate: 0.000030091\n",
      "train_loss: 0.061375, valid_loss: 0.046684\n",
      "train_f1: 0.921421, valid_f1: 0.939517\n",
      "--- 28.388572692871094 seconds ---\n",
      "Epoch : 136\n",
      "learning_rate: 0.000026243\n",
      "train_loss: 0.061083, valid_loss: 0.046682\n",
      "train_f1: 0.921645, valid_f1: 0.939478\n",
      "--- 28.412104845046997 seconds ---\n",
      "Epoch : 137\n",
      "learning_rate: 0.000022651\n",
      "train_loss: 0.061008, valid_loss: 0.046665\n",
      "train_f1: 0.921660, valid_f1: 0.939539\n",
      "--- 28.36382818222046 seconds ---\n",
      "Epoch : 138\n",
      "learning_rate: 0.000019318\n",
      "train_loss: 0.061147, valid_loss: 0.046697\n",
      "train_f1: 0.921519, valid_f1: 0.939443\n",
      "--- 28.307124137878418 seconds ---\n",
      "Epoch : 139\n",
      "learning_rate: 0.000016246\n",
      "train_loss: 0.061489, valid_loss: 0.046681\n",
      "train_f1: 0.921360, valid_f1: 0.939461\n",
      "--- 28.31665873527527 seconds ---\n",
      "Epoch : 140\n",
      "learning_rate: 0.000013435\n",
      "train_loss: 0.061119, valid_loss: 0.046689\n",
      "train_f1: 0.921413, valid_f1: 0.939454\n",
      "--- 28.461150884628296 seconds ---\n",
      "Epoch : 141\n",
      "learning_rate: 0.000010888\n",
      "train_loss: 0.061656, valid_loss: 0.046682\n",
      "train_f1: 0.921538, valid_f1: 0.939434\n",
      "--- 28.541791677474976 seconds ---\n",
      "Epoch : 142\n",
      "learning_rate: 0.000008606\n",
      "train_loss: 0.061782, valid_loss: 0.046680\n",
      "train_f1: 0.921595, valid_f1: 0.939513\n",
      "--- 28.5946524143219 seconds ---\n",
      "Epoch : 143\n",
      "learning_rate: 0.000006589\n",
      "train_loss: 0.060954, valid_loss: 0.046677\n",
      "train_f1: 0.921521, valid_f1: 0.939497\n",
      "--- 28.60675835609436 seconds ---\n",
      "Epoch : 144\n",
      "learning_rate: 0.000004840\n",
      "train_loss: 0.061388, valid_loss: 0.046683\n",
      "train_f1: 0.921682, valid_f1: 0.939502\n",
      "--- 28.515361070632935 seconds ---\n",
      "Epoch : 145\n",
      "learning_rate: 0.000003360\n",
      "train_loss: 0.061274, valid_loss: 0.046682\n",
      "train_f1: 0.921620, valid_f1: 0.939497\n",
      "Early Stopping...\n",
      "Best Val Score: 0.939606\n",
      "Fold : 2\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "train_loss: 2.031785, valid_loss: 1.766784\n",
      "train_f1: 0.056726, valid_f1: 0.087301\n",
      "--- 28.361382007598877 seconds ---\n",
      "Epoch : 1\n",
      "learning_rate: 0.000011938\n",
      "train_loss: 1.905249, valid_loss: 1.657732\n",
      "train_f1: 0.061216, valid_f1: 0.087794\n",
      "--- 28.56240487098694 seconds ---\n",
      "Epoch : 2\n",
      "learning_rate: 0.000044274\n",
      "train_loss: 1.680470, valid_loss: 1.390436\n",
      "train_f1: 0.076377, valid_f1: 0.047495\n",
      "--- 28.59905767440796 seconds ---\n",
      "Epoch : 3\n",
      "learning_rate: 0.000096592\n",
      "train_loss: 1.355948, valid_loss: 0.872988\n",
      "train_f1: 0.099509, valid_f1: 0.168042\n",
      "--- 28.380378484725952 seconds ---\n",
      "Epoch : 4\n",
      "learning_rate: 0.000166599\n",
      "train_loss: 0.994725, valid_loss: 0.621801\n",
      "train_f1: 0.144973, valid_f1: 0.251595\n",
      "--- 28.738038539886475 seconds ---\n",
      "Epoch : 5\n",
      "learning_rate: 0.000251230\n",
      "train_loss: 0.766174, valid_loss: 0.480884\n",
      "train_f1: 0.222007, valid_f1: 0.323152\n",
      "--- 28.495476961135864 seconds ---\n",
      "Epoch : 6\n",
      "learning_rate: 0.000346779\n",
      "train_loss: 0.599046, valid_loss: 0.397489\n",
      "train_f1: 0.299318, valid_f1: 0.397250\n",
      "--- 28.47497034072876 seconds ---\n",
      "Epoch : 7\n",
      "learning_rate: 0.000449060\n",
      "train_loss: 0.495155, valid_loss: 0.339824\n",
      "train_f1: 0.351484, valid_f1: 0.432967\n",
      "--- 28.54927659034729 seconds ---\n",
      "Epoch : 8\n",
      "learning_rate: 0.000553594\n",
      "train_loss: 0.415107, valid_loss: 0.273462\n",
      "train_f1: 0.416642, valid_f1: 0.464825\n",
      "--- 28.51239585876465 seconds ---\n",
      "Epoch : 9\n",
      "learning_rate: 0.000655802\n",
      "train_loss: 0.346638, valid_loss: 0.208807\n",
      "train_f1: 0.489470, valid_f1: 0.679641\n",
      "--- 28.470880270004272 seconds ---\n",
      "Epoch : 10\n",
      "learning_rate: 0.000751209\n",
      "train_loss: 0.283403, valid_loss: 0.159024\n",
      "train_f1: 0.565651, valid_f1: 0.779209\n",
      "--- 28.502651691436768 seconds ---\n",
      "Epoch : 11\n",
      "learning_rate: 0.000835636\n",
      "train_loss: 0.234454, valid_loss: 0.128114\n",
      "train_f1: 0.623568, valid_f1: 0.812230\n",
      "--- 28.719197750091553 seconds ---\n",
      "Epoch : 12\n",
      "learning_rate: 0.000905384\n",
      "train_loss: 0.200143, valid_loss: 0.106790\n",
      "train_f1: 0.670585, valid_f1: 0.830016\n",
      "--- 28.492380142211914 seconds ---\n",
      "Epoch : 13\n",
      "learning_rate: 0.000957400\n",
      "train_loss: 0.175873, valid_loss: 0.092062\n",
      "train_f1: 0.710568, valid_f1: 0.830785\n",
      "--- 28.373228073120117 seconds ---\n",
      "Epoch : 14\n",
      "learning_rate: 0.000989405\n",
      "train_loss: 0.157549, valid_loss: 0.084370\n",
      "train_f1: 0.739753, valid_f1: 0.836956\n",
      "--- 28.43755793571472 seconds ---\n",
      "Epoch : 15\n",
      "learning_rate: 0.001000000\n",
      "train_loss: 0.144287, valid_loss: 0.077044\n",
      "train_f1: 0.763617, valid_f1: 0.839235\n",
      "--- 28.365500450134277 seconds ---\n",
      "Epoch : 16\n",
      "learning_rate: 0.000999860\n",
      "train_loss: 0.134840, valid_loss: 0.073468\n",
      "train_f1: 0.780933, valid_f1: 0.842262\n",
      "--- 28.687262058258057 seconds ---\n",
      "Epoch : 17\n",
      "learning_rate: 0.000999450\n",
      "train_loss: 0.125652, valid_loss: 0.071045\n",
      "train_f1: 0.792216, valid_f1: 0.841535\n",
      "--- 28.364420175552368 seconds ---\n",
      "Epoch : 18\n",
      "learning_rate: 0.000998769\n",
      "train_loss: 0.120200, valid_loss: 0.073713\n",
      "train_f1: 0.802537, valid_f1: 0.833415\n",
      "--- 28.648963928222656 seconds ---\n",
      "Epoch : 19\n",
      "learning_rate: 0.000997818\n",
      "train_loss: 0.117417, valid_loss: 0.067551\n",
      "train_f1: 0.805860, valid_f1: 0.845722\n",
      "--- 28.63008213043213 seconds ---\n",
      "Epoch : 20\n",
      "learning_rate: 0.000996598\n",
      "train_loss: 0.111488, valid_loss: 0.066268\n",
      "train_f1: 0.813214, valid_f1: 0.845790\n",
      "--- 28.661535263061523 seconds ---\n",
      "Epoch : 21\n",
      "learning_rate: 0.000995108\n",
      "train_loss: 0.107770, valid_loss: 0.065384\n",
      "train_f1: 0.817131, valid_f1: 0.845537\n",
      "--- 28.45331048965454 seconds ---\n",
      "Epoch : 22\n",
      "learning_rate: 0.000993351\n",
      "train_loss: 0.104504, valid_loss: 0.064826\n",
      "train_f1: 0.820254, valid_f1: 0.846116\n",
      "--- 28.594107389450073 seconds ---\n",
      "Epoch : 23\n",
      "learning_rate: 0.000991326\n",
      "train_loss: 0.102280, valid_loss: 0.065162\n",
      "train_f1: 0.823009, valid_f1: 0.840895\n",
      "--- 28.5761935710907 seconds ---\n",
      "Epoch : 24\n",
      "learning_rate: 0.000989035\n",
      "train_loss: 0.100868, valid_loss: 0.062840\n",
      "train_f1: 0.825174, valid_f1: 0.845767\n",
      "--- 28.484126329421997 seconds ---\n",
      "Epoch : 25\n",
      "learning_rate: 0.000986480\n",
      "train_loss: 0.098702, valid_loss: 0.062104\n",
      "train_f1: 0.827583, valid_f1: 0.847238\n",
      "--- 28.516212463378906 seconds ---\n",
      "Epoch : 26\n",
      "learning_rate: 0.000983661\n",
      "train_loss: 0.096775, valid_loss: 0.061365\n",
      "train_f1: 0.830059, valid_f1: 0.846732\n",
      "--- 28.60081672668457 seconds ---\n",
      "Epoch : 27\n",
      "learning_rate: 0.000980580\n",
      "train_loss: 0.094726, valid_loss: 0.060971\n",
      "train_f1: 0.832175, valid_f1: 0.846684\n",
      "--- 28.312079668045044 seconds ---\n",
      "Epoch : 28\n",
      "learning_rate: 0.000977239\n",
      "train_loss: 0.093333, valid_loss: 0.060198\n",
      "train_f1: 0.834340, valid_f1: 0.847182\n",
      "--- 28.433740615844727 seconds ---\n",
      "Epoch : 29\n",
      "learning_rate: 0.000973639\n",
      "train_loss: 0.091304, valid_loss: 0.059506\n",
      "train_f1: 0.837406, valid_f1: 0.847397\n",
      "--- 28.2484769821167 seconds ---\n",
      "Epoch : 30\n",
      "learning_rate: 0.000969783\n",
      "train_loss: 0.090265, valid_loss: 0.059275\n",
      "train_f1: 0.842084, valid_f1: 0.846353\n",
      "--- 28.374757766723633 seconds ---\n",
      "Epoch : 31\n",
      "learning_rate: 0.000965673\n",
      "train_loss: 0.089811, valid_loss: 0.058596\n",
      "train_f1: 0.845136, valid_f1: 0.846890\n",
      "--- 28.639055967330933 seconds ---\n",
      "Epoch : 32\n",
      "learning_rate: 0.000961310\n",
      "train_loss: 0.089247, valid_loss: 0.057306\n",
      "train_f1: 0.848762, valid_f1: 0.848314\n",
      "--- 28.22363591194153 seconds ---\n",
      "Epoch : 33\n",
      "learning_rate: 0.000956698\n",
      "train_loss: 0.086474, valid_loss: 0.056872\n",
      "train_f1: 0.856312, valid_f1: 0.871064\n",
      "--- 28.322165966033936 seconds ---\n",
      "Epoch : 34\n",
      "learning_rate: 0.000951838\n",
      "train_loss: 0.085572, valid_loss: 0.056177\n",
      "train_f1: 0.861579, valid_f1: 0.921743\n",
      "--- 28.645501375198364 seconds ---\n",
      "Epoch : 35\n",
      "learning_rate: 0.000946733\n",
      "train_loss: 0.084154, valid_loss: 0.055480\n",
      "train_f1: 0.867758, valid_f1: 0.930136\n",
      "--- 28.474913358688354 seconds ---\n",
      "Epoch : 36\n",
      "learning_rate: 0.000941387\n",
      "train_loss: 0.082750, valid_loss: 0.055579\n",
      "train_f1: 0.873213, valid_f1: 0.930394\n",
      "--- 28.338717222213745 seconds ---\n",
      "Epoch : 37\n",
      "learning_rate: 0.000935802\n",
      "train_loss: 0.082080, valid_loss: 0.054724\n",
      "train_f1: 0.876357, valid_f1: 0.932482\n",
      "--- 28.387848615646362 seconds ---\n",
      "Epoch : 38\n",
      "learning_rate: 0.000929980\n",
      "train_loss: 0.082183, valid_loss: 0.054230\n",
      "train_f1: 0.880176, valid_f1: 0.932632\n",
      "--- 28.527982711791992 seconds ---\n",
      "Epoch : 39\n",
      "learning_rate: 0.000923926\n",
      "train_loss: 0.080774, valid_loss: 0.053921\n",
      "train_f1: 0.883114, valid_f1: 0.932624\n",
      "--- 28.33781599998474 seconds ---\n",
      "Epoch : 40\n",
      "learning_rate: 0.000917642\n",
      "train_loss: 0.096831, valid_loss: 0.056645\n",
      "train_f1: 0.861374, valid_f1: 0.931711\n",
      "--- 28.384194135665894 seconds ---\n",
      "Epoch : 41\n",
      "learning_rate: 0.000911132\n",
      "train_loss: 0.081936, valid_loss: 0.054373\n",
      "train_f1: 0.882694, valid_f1: 0.933420\n",
      "--- 28.244791746139526 seconds ---\n",
      "Epoch : 42\n",
      "learning_rate: 0.000904400\n",
      "train_loss: 0.079653, valid_loss: 0.053954\n",
      "train_f1: 0.887299, valid_f1: 0.933535\n",
      "--- 28.390963315963745 seconds ---\n",
      "Epoch : 43\n",
      "learning_rate: 0.000897448\n",
      "train_loss: 0.078825, valid_loss: 0.053377\n",
      "train_f1: 0.889158, valid_f1: 0.934293\n",
      "--- 28.56938123703003 seconds ---\n",
      "Epoch : 44\n",
      "learning_rate: 0.000890282\n",
      "train_loss: 0.077777, valid_loss: 0.053262\n",
      "train_f1: 0.891662, valid_f1: 0.933582\n",
      "--- 28.719505310058594 seconds ---\n",
      "Epoch : 45\n",
      "learning_rate: 0.000882903\n",
      "train_loss: 0.077534, valid_loss: 0.053054\n",
      "train_f1: 0.893235, valid_f1: 0.934802\n",
      "--- 28.650092363357544 seconds ---\n",
      "Epoch : 46\n",
      "learning_rate: 0.000875318\n",
      "train_loss: 0.077043, valid_loss: 0.052802\n",
      "train_f1: 0.894446, valid_f1: 0.934375\n",
      "--- 28.493638515472412 seconds ---\n",
      "Epoch : 47\n",
      "learning_rate: 0.000867529\n",
      "train_loss: 0.077012, valid_loss: 0.052767\n",
      "train_f1: 0.896253, valid_f1: 0.935155\n",
      "--- 28.503190517425537 seconds ---\n",
      "Epoch : 48\n",
      "learning_rate: 0.000859542\n",
      "train_loss: 0.075600, valid_loss: 0.052227\n",
      "train_f1: 0.897990, valid_f1: 0.935504\n",
      "--- 28.512467622756958 seconds ---\n",
      "Epoch : 49\n",
      "learning_rate: 0.000851359\n",
      "train_loss: 0.075126, valid_loss: 0.052086\n",
      "train_f1: 0.898650, valid_f1: 0.935184\n",
      "--- 28.65232491493225 seconds ---\n",
      "Epoch : 50\n",
      "learning_rate: 0.000842986\n",
      "train_loss: 0.074498, valid_loss: 0.051877\n",
      "train_f1: 0.900920, valid_f1: 0.934513\n",
      "--- 28.473594188690186 seconds ---\n",
      "Epoch : 51\n",
      "learning_rate: 0.000834428\n",
      "train_loss: 0.073986, valid_loss: 0.051603\n",
      "train_f1: 0.901958, valid_f1: 0.935327\n",
      "--- 28.091336011886597 seconds ---\n",
      "Epoch : 52\n",
      "learning_rate: 0.000825689\n",
      "train_loss: 0.073525, valid_loss: 0.051640\n",
      "train_f1: 0.902864, valid_f1: 0.935559\n",
      "--- 28.462478160858154 seconds ---\n",
      "Epoch : 53\n",
      "learning_rate: 0.000816773\n",
      "train_loss: 0.072575, valid_loss: 0.051214\n",
      "train_f1: 0.903751, valid_f1: 0.935706\n",
      "--- 28.426440000534058 seconds ---\n",
      "Epoch : 54\n",
      "learning_rate: 0.000807685\n",
      "train_loss: 0.072095, valid_loss: 0.051087\n",
      "train_f1: 0.905028, valid_f1: 0.935013\n",
      "--- 28.40617346763611 seconds ---\n",
      "Epoch : 55\n",
      "learning_rate: 0.000798431\n",
      "train_loss: 0.071960, valid_loss: 0.051540\n",
      "train_f1: 0.905990, valid_f1: 0.935395\n",
      "--- 28.47927737236023 seconds ---\n",
      "Epoch : 56\n",
      "learning_rate: 0.000789015\n",
      "train_loss: 0.071924, valid_loss: 0.051313\n",
      "train_f1: 0.906562, valid_f1: 0.934386\n",
      "--- 28.32575559616089 seconds ---\n",
      "Epoch : 57\n",
      "learning_rate: 0.000779443\n",
      "train_loss: 0.071828, valid_loss: 0.050821\n",
      "train_f1: 0.906825, valid_f1: 0.935870\n",
      "--- 28.304001331329346 seconds ---\n",
      "Epoch : 58\n",
      "learning_rate: 0.000769720\n",
      "train_loss: 0.070692, valid_loss: 0.050651\n",
      "train_f1: 0.907512, valid_f1: 0.936213\n",
      "--- 28.381752252578735 seconds ---\n",
      "Epoch : 59\n",
      "learning_rate: 0.000759850\n",
      "train_loss: 0.070785, valid_loss: 0.051151\n",
      "train_f1: 0.908204, valid_f1: 0.934588\n",
      "--- 28.37740468978882 seconds ---\n",
      "Epoch : 60\n",
      "learning_rate: 0.000749840\n",
      "train_loss: 0.070803, valid_loss: 0.050689\n",
      "train_f1: 0.909020, valid_f1: 0.936191\n",
      "--- 28.21259641647339 seconds ---\n",
      "Epoch : 61\n",
      "learning_rate: 0.000739695\n",
      "train_loss: 0.070082, valid_loss: 0.050622\n",
      "train_f1: 0.909765, valid_f1: 0.936324\n",
      "--- 28.566142082214355 seconds ---\n",
      "Epoch : 62\n",
      "learning_rate: 0.000729419\n",
      "train_loss: 0.069816, valid_loss: 0.050873\n",
      "train_f1: 0.909289, valid_f1: 0.932343\n",
      "--- 28.37292718887329 seconds ---\n",
      "Epoch : 63\n",
      "learning_rate: 0.000719020\n",
      "train_loss: 0.069110, valid_loss: 0.050278\n",
      "train_f1: 0.910505, valid_f1: 0.936313\n",
      "--- 28.36554527282715 seconds ---\n",
      "Epoch : 64\n",
      "learning_rate: 0.000708501\n",
      "train_loss: 0.068918, valid_loss: 0.050284\n",
      "train_f1: 0.911702, valid_f1: 0.935613\n",
      "--- 28.49764895439148 seconds ---\n",
      "Epoch : 65\n",
      "learning_rate: 0.000697870\n",
      "train_loss: 0.069089, valid_loss: 0.050133\n",
      "train_f1: 0.911571, valid_f1: 0.936254\n",
      "--- 28.336913347244263 seconds ---\n",
      "Epoch : 66\n",
      "learning_rate: 0.000687132\n",
      "train_loss: 0.068702, valid_loss: 0.050197\n",
      "train_f1: 0.912227, valid_f1: 0.936320\n",
      "--- 28.464019298553467 seconds ---\n",
      "Epoch : 67\n",
      "learning_rate: 0.000676292\n",
      "train_loss: 0.068022, valid_loss: 0.050114\n",
      "train_f1: 0.912394, valid_f1: 0.936213\n",
      "--- 28.626579761505127 seconds ---\n",
      "Epoch : 68\n",
      "learning_rate: 0.000665357\n",
      "train_loss: 0.068111, valid_loss: 0.049943\n",
      "train_f1: 0.912446, valid_f1: 0.936274\n",
      "--- 28.330779790878296 seconds ---\n",
      "Epoch : 69\n",
      "learning_rate: 0.000654333\n",
      "train_loss: 0.067773, valid_loss: 0.050036\n",
      "train_f1: 0.912604, valid_f1: 0.936134\n",
      "--- 28.291752576828003 seconds ---\n",
      "Epoch : 70\n",
      "learning_rate: 0.000643225\n",
      "train_loss: 0.068700, valid_loss: 0.049920\n",
      "train_f1: 0.913125, valid_f1: 0.936542\n",
      "--- 28.51406741142273 seconds ---\n",
      "Epoch : 71\n",
      "learning_rate: 0.000632039\n",
      "train_loss: 0.067445, valid_loss: 0.049817\n",
      "train_f1: 0.913054, valid_f1: 0.935832\n",
      "--- 28.346492052078247 seconds ---\n",
      "Epoch : 72\n",
      "learning_rate: 0.000620782\n",
      "train_loss: 0.066912, valid_loss: 0.049686\n",
      "train_f1: 0.913920, valid_f1: 0.936523\n",
      "--- 28.55743718147278 seconds ---\n",
      "Epoch : 73\n",
      "learning_rate: 0.000609459\n",
      "train_loss: 0.067056, valid_loss: 0.049646\n",
      "train_f1: 0.914043, valid_f1: 0.936487\n",
      "--- 28.426193475723267 seconds ---\n",
      "Epoch : 74\n",
      "learning_rate: 0.000598077\n",
      "train_loss: 0.066572, valid_loss: 0.049612\n",
      "train_f1: 0.914733, valid_f1: 0.936451\n",
      "--- 28.423807859420776 seconds ---\n",
      "Epoch : 75\n",
      "learning_rate: 0.000586642\n",
      "train_loss: 0.067123, valid_loss: 0.049376\n",
      "train_f1: 0.914649, valid_f1: 0.936312\n",
      "--- 28.45840620994568 seconds ---\n",
      "Epoch : 76\n",
      "learning_rate: 0.000575160\n",
      "train_loss: 0.066114, valid_loss: 0.049601\n",
      "train_f1: 0.915145, valid_f1: 0.936538\n",
      "--- 28.532232761383057 seconds ---\n",
      "Epoch : 77\n",
      "learning_rate: 0.000563638\n",
      "train_loss: 0.066295, valid_loss: 0.049446\n",
      "train_f1: 0.914885, valid_f1: 0.936855\n",
      "--- 28.591596603393555 seconds ---\n",
      "Epoch : 78\n",
      "learning_rate: 0.000552081\n",
      "train_loss: 0.065645, valid_loss: 0.049250\n",
      "train_f1: 0.915253, valid_f1: 0.936781\n",
      "--- 28.351806163787842 seconds ---\n",
      "Epoch : 79\n",
      "learning_rate: 0.000540495\n",
      "train_loss: 0.065534, valid_loss: 0.049490\n",
      "train_f1: 0.915886, valid_f1: 0.936332\n",
      "--- 28.18024468421936 seconds ---\n",
      "Epoch : 80\n",
      "learning_rate: 0.000528888\n",
      "train_loss: 0.065680, valid_loss: 0.049270\n",
      "train_f1: 0.916209, valid_f1: 0.936364\n",
      "--- 28.158841371536255 seconds ---\n",
      "Epoch : 81\n",
      "learning_rate: 0.000517265\n",
      "train_loss: 0.065171, valid_loss: 0.049309\n",
      "train_f1: 0.916486, valid_f1: 0.936634\n",
      "--- 28.491559743881226 seconds ---\n",
      "Epoch : 82\n",
      "learning_rate: 0.000505633\n",
      "train_loss: 0.065446, valid_loss: 0.049110\n",
      "train_f1: 0.916600, valid_f1: 0.936733\n",
      "--- 28.276952266693115 seconds ---\n",
      "Epoch : 83\n",
      "learning_rate: 0.000493998\n",
      "train_loss: 0.065781, valid_loss: 0.049297\n",
      "train_f1: 0.915430, valid_f1: 0.936779\n",
      "--- 28.192096710205078 seconds ---\n",
      "Epoch : 84\n",
      "learning_rate: 0.000482366\n",
      "train_loss: 0.064729, valid_loss: 0.049256\n",
      "train_f1: 0.916909, valid_f1: 0.936412\n",
      "--- 28.30890941619873 seconds ---\n",
      "Epoch : 85\n",
      "learning_rate: 0.000470743\n",
      "train_loss: 0.064666, valid_loss: 0.049233\n",
      "train_f1: 0.917076, valid_f1: 0.936806\n",
      "--- 28.76772975921631 seconds ---\n",
      "Epoch : 86\n",
      "learning_rate: 0.000459137\n",
      "train_loss: 0.065078, valid_loss: 0.049089\n",
      "train_f1: 0.917330, valid_f1: 0.936525\n",
      "--- 28.236871004104614 seconds ---\n",
      "Epoch : 87\n",
      "learning_rate: 0.000447552\n",
      "train_loss: 0.064901, valid_loss: 0.049112\n",
      "train_f1: 0.917309, valid_f1: 0.936726\n",
      "--- 28.26017165184021 seconds ---\n",
      "Epoch : 88\n",
      "learning_rate: 0.000435996\n",
      "train_loss: 0.064155, valid_loss: 0.049076\n",
      "train_f1: 0.917414, valid_f1: 0.936818\n",
      "--- 28.379495859146118 seconds ---\n",
      "Epoch : 89\n",
      "learning_rate: 0.000424475\n",
      "train_loss: 0.064192, valid_loss: 0.048952\n",
      "train_f1: 0.917734, valid_f1: 0.936751\n",
      "--- 28.357370138168335 seconds ---\n",
      "Epoch : 90\n",
      "learning_rate: 0.000412994\n",
      "train_loss: 0.064643, valid_loss: 0.049101\n",
      "train_f1: 0.916981, valid_f1: 0.936610\n",
      "--- 28.374177932739258 seconds ---\n",
      "Epoch : 91\n",
      "learning_rate: 0.000401561\n",
      "train_loss: 0.063702, valid_loss: 0.048970\n",
      "train_f1: 0.918122, valid_f1: 0.936667\n",
      "--- 28.350855827331543 seconds ---\n",
      "Epoch : 92\n",
      "learning_rate: 0.000390181\n",
      "train_loss: 0.063333, valid_loss: 0.048855\n",
      "train_f1: 0.917928, valid_f1: 0.937120\n",
      "--- 28.36177682876587 seconds ---\n",
      "Epoch : 93\n",
      "learning_rate: 0.000378860\n",
      "train_loss: 0.063968, valid_loss: 0.049001\n",
      "train_f1: 0.918308, valid_f1: 0.935505\n",
      "--- 28.394503831863403 seconds ---\n",
      "Epoch : 94\n",
      "learning_rate: 0.000367605\n",
      "train_loss: 0.063310, valid_loss: 0.048663\n",
      "train_f1: 0.918488, valid_f1: 0.936967\n",
      "--- 28.480166912078857 seconds ---\n",
      "Epoch : 95\n",
      "learning_rate: 0.000356422\n",
      "train_loss: 0.063289, valid_loss: 0.048866\n",
      "train_f1: 0.918594, valid_f1: 0.937042\n",
      "--- 28.434508323669434 seconds ---\n",
      "Epoch : 96\n",
      "learning_rate: 0.000345316\n",
      "train_loss: 0.063155, valid_loss: 0.048798\n",
      "train_f1: 0.918993, valid_f1: 0.936866\n",
      "--- 28.38319706916809 seconds ---\n",
      "Epoch : 97\n",
      "learning_rate: 0.000334294\n",
      "train_loss: 0.063269, valid_loss: 0.048693\n",
      "train_f1: 0.918951, valid_f1: 0.936977\n",
      "--- 28.43419909477234 seconds ---\n",
      "Epoch : 98\n",
      "learning_rate: 0.000323362\n",
      "train_loss: 0.063092, valid_loss: 0.048697\n",
      "train_f1: 0.918962, valid_f1: 0.937019\n",
      "--- 28.26253867149353 seconds ---\n",
      "Epoch : 99\n",
      "learning_rate: 0.000312526\n",
      "train_loss: 0.062856, valid_loss: 0.048521\n",
      "train_f1: 0.918965, valid_f1: 0.937111\n",
      "--- 28.529427766799927 seconds ---\n",
      "Epoch : 100\n",
      "learning_rate: 0.000301791\n",
      "train_loss: 0.063274, valid_loss: 0.048611\n",
      "train_f1: 0.919563, valid_f1: 0.936583\n",
      "--- 28.431880235671997 seconds ---\n",
      "Epoch : 101\n",
      "learning_rate: 0.000291163\n",
      "train_loss: 0.062666, valid_loss: 0.048649\n",
      "train_f1: 0.919323, valid_f1: 0.937033\n",
      "--- 28.21689224243164 seconds ---\n",
      "Epoch : 102\n",
      "learning_rate: 0.000280649\n",
      "train_loss: 0.062934, valid_loss: 0.048607\n",
      "train_f1: 0.919441, valid_f1: 0.937109\n",
      "--- 28.183650255203247 seconds ---\n",
      "Epoch : 103\n",
      "learning_rate: 0.000270253\n",
      "train_loss: 0.062660, valid_loss: 0.048547\n",
      "train_f1: 0.919520, valid_f1: 0.936996\n",
      "--- 28.224730014801025 seconds ---\n",
      "Epoch : 104\n",
      "learning_rate: 0.000259981\n",
      "train_loss: 0.062100, valid_loss: 0.048644\n",
      "train_f1: 0.919747, valid_f1: 0.936346\n",
      "--- 28.189812898635864 seconds ---\n",
      "Epoch : 105\n",
      "learning_rate: 0.000249840\n",
      "train_loss: 0.062180, valid_loss: 0.048685\n",
      "train_f1: 0.919817, valid_f1: 0.936992\n",
      "--- 28.12641453742981 seconds ---\n",
      "Epoch : 106\n",
      "learning_rate: 0.000239834\n",
      "train_loss: 0.062276, valid_loss: 0.048431\n",
      "train_f1: 0.919802, valid_f1: 0.936838\n",
      "--- 28.137875080108643 seconds ---\n",
      "Epoch : 107\n",
      "learning_rate: 0.000229969\n",
      "train_loss: 0.062415, valid_loss: 0.048417\n",
      "train_f1: 0.920175, valid_f1: 0.937130\n",
      "--- 28.105887174606323 seconds ---\n",
      "Epoch : 108\n",
      "learning_rate: 0.000220251\n",
      "train_loss: 0.062294, valid_loss: 0.048496\n",
      "train_f1: 0.919930, valid_f1: 0.936998\n",
      "--- 28.271132946014404 seconds ---\n",
      "Epoch : 109\n",
      "learning_rate: 0.000210683\n",
      "train_loss: 0.061861, valid_loss: 0.048516\n",
      "train_f1: 0.920463, valid_f1: 0.936952\n",
      "--- 28.342762231826782 seconds ---\n",
      "Epoch : 110\n",
      "learning_rate: 0.000201273\n",
      "train_loss: 0.062168, valid_loss: 0.048407\n",
      "train_f1: 0.920220, valid_f1: 0.937041\n",
      "--- 28.576260805130005 seconds ---\n",
      "Epoch : 111\n",
      "learning_rate: 0.000192024\n",
      "train_loss: 0.061890, valid_loss: 0.048394\n",
      "train_f1: 0.920525, valid_f1: 0.937073\n",
      "--- 28.352508783340454 seconds ---\n",
      "Epoch : 112\n",
      "learning_rate: 0.000182942\n",
      "train_loss: 0.062087, valid_loss: 0.048393\n",
      "train_f1: 0.920500, valid_f1: 0.937221\n",
      "--- 28.53093671798706 seconds ---\n",
      "Epoch : 113\n",
      "learning_rate: 0.000174031\n",
      "train_loss: 0.062298, valid_loss: 0.048332\n",
      "train_f1: 0.920628, valid_f1: 0.936524\n",
      "--- 28.58866262435913 seconds ---\n",
      "Epoch : 114\n",
      "learning_rate: 0.000165298\n",
      "train_loss: 0.061665, valid_loss: 0.048405\n",
      "train_f1: 0.920515, valid_f1: 0.936840\n",
      "--- 28.204277753829956 seconds ---\n",
      "Epoch : 115\n",
      "learning_rate: 0.000156745\n",
      "train_loss: 0.061771, valid_loss: 0.048248\n",
      "train_f1: 0.920717, valid_f1: 0.937178\n",
      "--- 28.190232515335083 seconds ---\n",
      "Epoch : 116\n",
      "learning_rate: 0.000148378\n",
      "train_loss: 0.061656, valid_loss: 0.048285\n",
      "train_f1: 0.920317, valid_f1: 0.936515\n",
      "--- 28.172384023666382 seconds ---\n",
      "Epoch : 117\n",
      "learning_rate: 0.000140202\n",
      "train_loss: 0.061671, valid_loss: 0.048312\n",
      "train_f1: 0.921037, valid_f1: 0.936986\n",
      "--- 28.076845169067383 seconds ---\n",
      "Epoch : 118\n",
      "learning_rate: 0.000132220\n",
      "train_loss: 0.061598, valid_loss: 0.048311\n",
      "train_f1: 0.920628, valid_f1: 0.937152\n",
      "--- 28.460031747817993 seconds ---\n",
      "Epoch : 119\n",
      "learning_rate: 0.000124438\n",
      "train_loss: 0.061410, valid_loss: 0.048230\n",
      "train_f1: 0.921143, valid_f1: 0.937168\n",
      "--- 28.292358875274658 seconds ---\n",
      "Epoch : 120\n",
      "learning_rate: 0.000116859\n",
      "train_loss: 0.061266, valid_loss: 0.048167\n",
      "train_f1: 0.920918, valid_f1: 0.937188\n",
      "--- 28.567946910858154 seconds ---\n",
      "Epoch : 121\n",
      "learning_rate: 0.000109488\n",
      "train_loss: 0.061522, valid_loss: 0.048277\n",
      "train_f1: 0.921090, valid_f1: 0.936918\n",
      "--- 28.43449854850769 seconds ---\n",
      "Epoch : 122\n",
      "learning_rate: 0.000102328\n",
      "train_loss: 0.061343, valid_loss: 0.048268\n",
      "train_f1: 0.921098, valid_f1: 0.936949\n",
      "--- 28.582985162734985 seconds ---\n",
      "Epoch : 123\n",
      "learning_rate: 0.000095383\n",
      "train_loss: 0.061710, valid_loss: 0.048192\n",
      "train_f1: 0.921082, valid_f1: 0.937226\n",
      "--- 28.404836654663086 seconds ---\n",
      "Epoch : 124\n",
      "learning_rate: 0.000088658\n",
      "train_loss: 0.061544, valid_loss: 0.048238\n",
      "train_f1: 0.921232, valid_f1: 0.937167\n",
      "--- 28.384877920150757 seconds ---\n",
      "Epoch : 125\n",
      "learning_rate: 0.000082155\n",
      "train_loss: 0.061281, valid_loss: 0.048234\n",
      "train_f1: 0.921304, valid_f1: 0.936992\n",
      "--- 28.316168069839478 seconds ---\n",
      "Epoch : 126\n",
      "learning_rate: 0.000075878\n",
      "train_loss: 0.061000, valid_loss: 0.048177\n",
      "train_f1: 0.921154, valid_f1: 0.937266\n",
      "--- 28.36150074005127 seconds ---\n",
      "Epoch : 127\n",
      "learning_rate: 0.000069831\n",
      "train_loss: 0.060864, valid_loss: 0.048249\n",
      "train_f1: 0.921424, valid_f1: 0.936786\n",
      "--- 28.275527000427246 seconds ---\n",
      "Epoch : 128\n",
      "learning_rate: 0.000064017\n",
      "train_loss: 0.060768, valid_loss: 0.048194\n",
      "train_f1: 0.921262, valid_f1: 0.936659\n",
      "--- 28.406975507736206 seconds ---\n",
      "Epoch : 129\n",
      "learning_rate: 0.000058440\n",
      "train_loss: 0.060988, valid_loss: 0.048208\n",
      "train_f1: 0.921453, valid_f1: 0.937017\n",
      "--- 28.359628200531006 seconds ---\n",
      "Epoch : 130\n",
      "learning_rate: 0.000053101\n",
      "train_loss: 0.060951, valid_loss: 0.048194\n",
      "train_f1: 0.921448, valid_f1: 0.936919\n",
      "--- 28.302382707595825 seconds ---\n",
      "Epoch : 131\n",
      "learning_rate: 0.000048004\n",
      "train_loss: 0.060962, valid_loss: 0.048152\n",
      "train_f1: 0.921371, valid_f1: 0.936978\n",
      "--- 28.318926572799683 seconds ---\n",
      "Epoch : 132\n",
      "learning_rate: 0.000043152\n",
      "train_loss: 0.061555, valid_loss: 0.048219\n",
      "train_f1: 0.921106, valid_f1: 0.936955\n",
      "--- 28.426745891571045 seconds ---\n",
      "Epoch : 133\n",
      "learning_rate: 0.000038548\n",
      "train_loss: 0.060824, valid_loss: 0.048179\n",
      "train_f1: 0.921415, valid_f1: 0.937097\n",
      "--- 28.34483027458191 seconds ---\n",
      "Epoch : 134\n",
      "learning_rate: 0.000034193\n",
      "train_loss: 0.061262, valid_loss: 0.048142\n",
      "train_f1: 0.921213, valid_f1: 0.937110\n",
      "--- 28.284960508346558 seconds ---\n",
      "Epoch : 135\n",
      "learning_rate: 0.000030091\n",
      "train_loss: 0.061091, valid_loss: 0.048131\n",
      "train_f1: 0.921679, valid_f1: 0.937170\n",
      "--- 28.368340730667114 seconds ---\n",
      "Epoch : 136\n",
      "learning_rate: 0.000026243\n",
      "train_loss: 0.060690, valid_loss: 0.048150\n",
      "train_f1: 0.921412, valid_f1: 0.937208\n",
      "--- 28.340860605239868 seconds ---\n",
      "Epoch : 137\n",
      "learning_rate: 0.000022651\n",
      "train_loss: 0.061419, valid_loss: 0.048156\n",
      "train_f1: 0.921451, valid_f1: 0.937162\n",
      "--- 28.333218574523926 seconds ---\n",
      "Epoch : 138\n",
      "learning_rate: 0.000019318\n",
      "train_loss: 0.060635, valid_loss: 0.048178\n",
      "train_f1: 0.921376, valid_f1: 0.936921\n",
      "--- 28.37449860572815 seconds ---\n",
      "Epoch : 139\n",
      "learning_rate: 0.000016246\n",
      "train_loss: 0.060805, valid_loss: 0.048164\n",
      "train_f1: 0.921435, valid_f1: 0.937166\n",
      "--- 28.191102504730225 seconds ---\n",
      "Epoch : 140\n",
      "learning_rate: 0.000013435\n",
      "train_loss: 0.060772, valid_loss: 0.048176\n",
      "train_f1: 0.921354, valid_f1: 0.937192\n",
      "--- 28.111013412475586 seconds ---\n",
      "Epoch : 141\n",
      "learning_rate: 0.000010888\n",
      "train_loss: 0.060891, valid_loss: 0.048133\n",
      "train_f1: 0.922054, valid_f1: 0.937197\n",
      "--- 28.113186836242676 seconds ---\n",
      "Epoch : 142\n",
      "learning_rate: 0.000008606\n",
      "train_loss: 0.060978, valid_loss: 0.048129\n",
      "train_f1: 0.921938, valid_f1: 0.937163\n",
      "--- 28.176352739334106 seconds ---\n",
      "Epoch : 143\n",
      "learning_rate: 0.000006589\n",
      "train_loss: 0.060982, valid_loss: 0.048117\n",
      "train_f1: 0.921669, valid_f1: 0.937148\n",
      "--- 28.240935564041138 seconds ---\n",
      "Epoch : 144\n",
      "learning_rate: 0.000004840\n",
      "train_loss: 0.061079, valid_loss: 0.048121\n",
      "train_f1: 0.921401, valid_f1: 0.937206\n",
      "--- 28.2453453540802 seconds ---\n",
      "Epoch : 145\n",
      "learning_rate: 0.000003360\n",
      "train_loss: 0.060903, valid_loss: 0.048126\n",
      "train_f1: 0.921591, valid_f1: 0.937214\n",
      "--- 28.261096000671387 seconds ---\n",
      "Epoch : 146\n",
      "learning_rate: 0.000002148\n",
      "train_loss: 0.061391, valid_loss: 0.048118\n",
      "train_f1: 0.921405, valid_f1: 0.937213\n",
      "Early Stopping...\n",
      "Best Val Score: 0.937266\n",
      "Fold : 3\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "train_loss: 2.492310, valid_loss: 2.207737\n",
      "train_f1: 0.032580, valid_f1: 0.001200\n",
      "--- 28.206918716430664 seconds ---\n",
      "Epoch : 1\n",
      "learning_rate: 0.000011938\n",
      "train_loss: 2.286658, valid_loss: 1.886573\n",
      "train_f1: 0.035784, valid_f1: 0.025721\n",
      "--- 28.459921836853027 seconds ---\n",
      "Epoch : 2\n",
      "learning_rate: 0.000044274\n",
      "train_loss: 1.874866, valid_loss: 1.412300\n",
      "train_f1: 0.043848, valid_f1: 0.013400\n",
      "--- 28.342094898223877 seconds ---\n",
      "Epoch : 3\n",
      "learning_rate: 0.000096592\n",
      "train_loss: 1.380534, valid_loss: 0.869009\n",
      "train_f1: 0.065891, valid_f1: 0.089392\n",
      "--- 28.436856031417847 seconds ---\n",
      "Epoch : 4\n",
      "learning_rate: 0.000166599\n",
      "train_loss: 1.029123, valid_loss: 0.684048\n",
      "train_f1: 0.129285, valid_f1: 0.180765\n",
      "--- 28.496652603149414 seconds ---\n",
      "Epoch : 5\n",
      "learning_rate: 0.000251230\n",
      "train_loss: 0.836973, valid_loss: 0.564796\n",
      "train_f1: 0.179169, valid_f1: 0.200554\n",
      "--- 28.625757217407227 seconds ---\n",
      "Epoch : 6\n",
      "learning_rate: 0.000346779\n",
      "train_loss: 0.684207, valid_loss: 0.460916\n",
      "train_f1: 0.253462, valid_f1: 0.308020\n",
      "--- 28.393239498138428 seconds ---\n",
      "Epoch : 7\n",
      "learning_rate: 0.000449060\n",
      "train_loss: 0.559139, valid_loss: 0.393641\n",
      "train_f1: 0.318975, valid_f1: 0.389699\n",
      "--- 28.52002501487732 seconds ---\n",
      "Epoch : 8\n",
      "learning_rate: 0.000553594\n",
      "train_loss: 0.472775, valid_loss: 0.336212\n",
      "train_f1: 0.367017, valid_f1: 0.423569\n",
      "--- 28.53789234161377 seconds ---\n",
      "Epoch : 9\n",
      "learning_rate: 0.000655802\n",
      "train_loss: 0.402537, valid_loss: 0.267712\n",
      "train_f1: 0.430612, valid_f1: 0.558420\n",
      "--- 28.26146697998047 seconds ---\n",
      "Epoch : 10\n",
      "learning_rate: 0.000751209\n",
      "train_loss: 0.336216, valid_loss: 0.200667\n",
      "train_f1: 0.512016, valid_f1: 0.732593\n",
      "--- 28.41020703315735 seconds ---\n",
      "Epoch : 11\n",
      "learning_rate: 0.000835636\n",
      "train_loss: 0.275682, valid_loss: 0.164419\n",
      "train_f1: 0.582957, valid_f1: 0.692227\n",
      "--- 28.415082454681396 seconds ---\n",
      "Epoch : 12\n",
      "learning_rate: 0.000905384\n",
      "train_loss: 0.231603, valid_loss: 0.128834\n",
      "train_f1: 0.637961, valid_f1: 0.813339\n",
      "--- 28.34264874458313 seconds ---\n",
      "Epoch : 13\n",
      "learning_rate: 0.000957400\n",
      "train_loss: 0.201105, valid_loss: 0.109478\n",
      "train_f1: 0.684354, valid_f1: 0.832457\n",
      "--- 28.32873225212097 seconds ---\n",
      "Epoch : 14\n",
      "learning_rate: 0.000989405\n",
      "train_loss: 0.178586, valid_loss: 0.097766\n",
      "train_f1: 0.721013, valid_f1: 0.837905\n",
      "--- 28.203848123550415 seconds ---\n",
      "Epoch : 15\n",
      "learning_rate: 0.001000000\n",
      "train_loss: 0.161035, valid_loss: 0.087269\n",
      "train_f1: 0.749102, valid_f1: 0.840499\n",
      "--- 28.401463985443115 seconds ---\n",
      "Epoch : 16\n",
      "learning_rate: 0.000999860\n",
      "train_loss: 0.146819, valid_loss: 0.080894\n",
      "train_f1: 0.768670, valid_f1: 0.842443\n",
      "--- 28.27840828895569 seconds ---\n",
      "Epoch : 17\n",
      "learning_rate: 0.000999450\n",
      "train_loss: 0.137924, valid_loss: 0.076400\n",
      "train_f1: 0.782866, valid_f1: 0.844083\n",
      "--- 28.416336059570312 seconds ---\n",
      "Epoch : 18\n",
      "learning_rate: 0.000998769\n",
      "train_loss: 0.130762, valid_loss: 0.073787\n",
      "train_f1: 0.793060, valid_f1: 0.845211\n",
      "--- 28.284212589263916 seconds ---\n",
      "Epoch : 19\n",
      "learning_rate: 0.000997818\n",
      "train_loss: 0.124682, valid_loss: 0.071174\n",
      "train_f1: 0.801261, valid_f1: 0.845801\n",
      "--- 28.350605726242065 seconds ---\n",
      "Epoch : 20\n",
      "learning_rate: 0.000996598\n",
      "train_loss: 0.119507, valid_loss: 0.069863\n",
      "train_f1: 0.809483, valid_f1: 0.846150\n",
      "--- 28.410850524902344 seconds ---\n",
      "Epoch : 21\n",
      "learning_rate: 0.000995108\n",
      "train_loss: 0.115317, valid_loss: 0.068612\n",
      "train_f1: 0.815713, valid_f1: 0.846149\n",
      "--- 28.37031316757202 seconds ---\n",
      "Epoch : 22\n",
      "learning_rate: 0.000993351\n",
      "train_loss: 0.112095, valid_loss: 0.068251\n",
      "train_f1: 0.819326, valid_f1: 0.846073\n",
      "--- 28.40061902999878 seconds ---\n",
      "Epoch : 23\n",
      "learning_rate: 0.000991326\n",
      "train_loss: 0.109313, valid_loss: 0.067063\n",
      "train_f1: 0.824197, valid_f1: 0.846826\n",
      "--- 28.173155784606934 seconds ---\n",
      "Epoch : 24\n",
      "learning_rate: 0.000989035\n",
      "train_loss: 0.105940, valid_loss: 0.065899\n",
      "train_f1: 0.828070, valid_f1: 0.847293\n",
      "--- 28.294546604156494 seconds ---\n",
      "Epoch : 25\n",
      "learning_rate: 0.000986480\n",
      "train_loss: 0.104381, valid_loss: 0.065241\n",
      "train_f1: 0.832235, valid_f1: 0.848158\n",
      "--- 28.362354040145874 seconds ---\n",
      "Epoch : 26\n",
      "learning_rate: 0.000983661\n",
      "train_loss: 0.101482, valid_loss: 0.064542\n",
      "train_f1: 0.835824, valid_f1: 0.847744\n",
      "--- 28.1783230304718 seconds ---\n",
      "Epoch : 27\n",
      "learning_rate: 0.000980580\n",
      "train_loss: 0.099495, valid_loss: 0.064017\n",
      "train_f1: 0.839516, valid_f1: 0.847521\n",
      "--- 28.264572858810425 seconds ---\n",
      "Epoch : 28\n",
      "learning_rate: 0.000977239\n",
      "train_loss: 0.098296, valid_loss: 0.063603\n",
      "train_f1: 0.844524, valid_f1: 0.846379\n",
      "--- 28.306853771209717 seconds ---\n",
      "Epoch : 29\n",
      "learning_rate: 0.000973639\n",
      "train_loss: 0.096943, valid_loss: 0.062302\n",
      "train_f1: 0.848707, valid_f1: 0.859239\n",
      "--- 28.189770936965942 seconds ---\n",
      "Epoch : 30\n",
      "learning_rate: 0.000969783\n",
      "train_loss: 0.094629, valid_loss: 0.061000\n",
      "train_f1: 0.854041, valid_f1: 0.898640\n",
      "--- 28.36667823791504 seconds ---\n",
      "Epoch : 31\n",
      "learning_rate: 0.000965673\n",
      "train_loss: 0.093060, valid_loss: 0.060372\n",
      "train_f1: 0.858669, valid_f1: 0.904245\n",
      "--- 28.340441465377808 seconds ---\n",
      "Epoch : 32\n",
      "learning_rate: 0.000961310\n",
      "train_loss: 0.091080, valid_loss: 0.060213\n",
      "train_f1: 0.863889, valid_f1: 0.925825\n",
      "--- 28.148375749588013 seconds ---\n",
      "Epoch : 33\n",
      "learning_rate: 0.000956698\n",
      "train_loss: 0.090849, valid_loss: 0.059189\n",
      "train_f1: 0.868664, valid_f1: 0.927629\n",
      "--- 28.3265380859375 seconds ---\n",
      "Epoch : 34\n",
      "learning_rate: 0.000951838\n",
      "train_loss: 0.089302, valid_loss: 0.059162\n",
      "train_f1: 0.872506, valid_f1: 0.921364\n",
      "--- 28.324182271957397 seconds ---\n",
      "Epoch : 35\n",
      "learning_rate: 0.000946733\n",
      "train_loss: 0.087991, valid_loss: 0.058188\n",
      "train_f1: 0.875719, valid_f1: 0.930316\n",
      "--- 28.342416524887085 seconds ---\n",
      "Epoch : 36\n",
      "learning_rate: 0.000941387\n",
      "train_loss: 0.086888, valid_loss: 0.057940\n",
      "train_f1: 0.878314, valid_f1: 0.932513\n",
      "--- 28.351295232772827 seconds ---\n",
      "Epoch : 37\n",
      "learning_rate: 0.000935802\n",
      "train_loss: 0.085929, valid_loss: 0.057623\n",
      "train_f1: 0.879902, valid_f1: 0.932270\n",
      "--- 28.314964532852173 seconds ---\n",
      "Epoch : 38\n",
      "learning_rate: 0.000929980\n",
      "train_loss: 0.085234, valid_loss: 0.056582\n",
      "train_f1: 0.883067, valid_f1: 0.933230\n",
      "--- 28.336031436920166 seconds ---\n",
      "Epoch : 39\n",
      "learning_rate: 0.000923926\n",
      "train_loss: 0.083814, valid_loss: 0.056558\n",
      "train_f1: 0.884803, valid_f1: 0.935183\n",
      "--- 28.22844672203064 seconds ---\n",
      "Epoch : 40\n",
      "learning_rate: 0.000917642\n",
      "train_loss: 0.083955, valid_loss: 0.057067\n",
      "train_f1: 0.886149, valid_f1: 0.930689\n",
      "--- 28.296261310577393 seconds ---\n",
      "Epoch : 41\n",
      "learning_rate: 0.000911132\n",
      "train_loss: 0.082327, valid_loss: 0.055606\n",
      "train_f1: 0.886880, valid_f1: 0.935767\n",
      "--- 28.201295375823975 seconds ---\n",
      "Epoch : 42\n",
      "learning_rate: 0.000904400\n",
      "train_loss: 0.082025, valid_loss: 0.055508\n",
      "train_f1: 0.889078, valid_f1: 0.935181\n",
      "--- 28.42561936378479 seconds ---\n",
      "Epoch : 43\n",
      "learning_rate: 0.000897448\n",
      "train_loss: 0.081537, valid_loss: 0.054975\n",
      "train_f1: 0.889240, valid_f1: 0.936583\n",
      "--- 28.333580017089844 seconds ---\n",
      "Epoch : 44\n",
      "learning_rate: 0.000890282\n",
      "train_loss: 0.080074, valid_loss: 0.055670\n",
      "train_f1: 0.890458, valid_f1: 0.931837\n",
      "--- 28.376331329345703 seconds ---\n",
      "Epoch : 45\n",
      "learning_rate: 0.000882903\n",
      "train_loss: 0.080219, valid_loss: 0.054719\n",
      "train_f1: 0.891367, valid_f1: 0.935576\n",
      "--- 28.2888503074646 seconds ---\n",
      "Epoch : 46\n",
      "learning_rate: 0.000875318\n",
      "train_loss: 0.079392, valid_loss: 0.054172\n",
      "train_f1: 0.892356, valid_f1: 0.936788\n",
      "--- 28.22269368171692 seconds ---\n",
      "Epoch : 47\n",
      "learning_rate: 0.000867529\n",
      "train_loss: 0.078346, valid_loss: 0.055100\n",
      "train_f1: 0.894024, valid_f1: 0.934249\n",
      "--- 28.303186655044556 seconds ---\n",
      "Epoch : 48\n",
      "learning_rate: 0.000859542\n",
      "train_loss: 0.078075, valid_loss: 0.054191\n",
      "train_f1: 0.896201, valid_f1: 0.933882\n",
      "--- 28.320772409439087 seconds ---\n",
      "Epoch : 49\n",
      "learning_rate: 0.000851359\n",
      "train_loss: 0.077188, valid_loss: 0.054088\n",
      "train_f1: 0.897583, valid_f1: 0.935752\n",
      "--- 28.319657564163208 seconds ---\n",
      "Epoch : 50\n",
      "learning_rate: 0.000842986\n",
      "train_loss: 0.076564, valid_loss: 0.053492\n",
      "train_f1: 0.899329, valid_f1: 0.936237\n",
      "--- 28.28963351249695 seconds ---\n",
      "Epoch : 51\n",
      "learning_rate: 0.000834428\n",
      "train_loss: 0.075974, valid_loss: 0.053164\n",
      "train_f1: 0.901390, valid_f1: 0.936103\n",
      "--- 28.36909770965576 seconds ---\n",
      "Epoch : 52\n",
      "learning_rate: 0.000825689\n",
      "train_loss: 0.075698, valid_loss: 0.053365\n",
      "train_f1: 0.902291, valid_f1: 0.936326\n",
      "--- 28.374869346618652 seconds ---\n",
      "Epoch : 53\n",
      "learning_rate: 0.000816773\n",
      "train_loss: 0.075464, valid_loss: 0.053077\n",
      "train_f1: 0.903154, valid_f1: 0.934864\n",
      "--- 28.335247039794922 seconds ---\n",
      "Epoch : 54\n",
      "learning_rate: 0.000807685\n",
      "train_loss: 0.074258, valid_loss: 0.053113\n",
      "train_f1: 0.904253, valid_f1: 0.935882\n",
      "--- 28.34710431098938 seconds ---\n",
      "Epoch : 55\n",
      "learning_rate: 0.000798431\n",
      "train_loss: 0.074547, valid_loss: 0.052924\n",
      "train_f1: 0.905046, valid_f1: 0.936463\n",
      "--- 28.281453371047974 seconds ---\n",
      "Epoch : 56\n",
      "learning_rate: 0.000789015\n",
      "train_loss: 0.073519, valid_loss: 0.052704\n",
      "train_f1: 0.905971, valid_f1: 0.934939\n",
      "--- 28.089312314987183 seconds ---\n",
      "Epoch : 57\n",
      "learning_rate: 0.000779443\n",
      "train_loss: 0.073676, valid_loss: 0.052630\n",
      "train_f1: 0.905433, valid_f1: 0.936847\n",
      "--- 28.235334634780884 seconds ---\n",
      "Epoch : 58\n",
      "learning_rate: 0.000769720\n",
      "train_loss: 0.073386, valid_loss: 0.052354\n",
      "train_f1: 0.906279, valid_f1: 0.936997\n",
      "--- 28.328625917434692 seconds ---\n",
      "Epoch : 59\n",
      "learning_rate: 0.000759850\n",
      "train_loss: 0.080968, valid_loss: 0.053335\n",
      "train_f1: 0.894330, valid_f1: 0.935772\n",
      "--- 28.13461661338806 seconds ---\n",
      "Epoch : 60\n",
      "learning_rate: 0.000749840\n",
      "train_loss: 0.073291, valid_loss: 0.052271\n",
      "train_f1: 0.906831, valid_f1: 0.936627\n",
      "--- 28.232271671295166 seconds ---\n",
      "Epoch : 61\n",
      "learning_rate: 0.000739695\n",
      "train_loss: 0.072094, valid_loss: 0.052134\n",
      "train_f1: 0.907995, valid_f1: 0.937026\n",
      "--- 28.212405920028687 seconds ---\n",
      "Epoch : 62\n",
      "learning_rate: 0.000729419\n",
      "train_loss: 0.071883, valid_loss: 0.052021\n",
      "train_f1: 0.908598, valid_f1: 0.936296\n",
      "--- 28.475202560424805 seconds ---\n",
      "Epoch : 63\n",
      "learning_rate: 0.000719020\n",
      "train_loss: 0.072035, valid_loss: 0.051720\n",
      "train_f1: 0.908604, valid_f1: 0.936747\n",
      "--- 28.287848472595215 seconds ---\n",
      "Epoch : 64\n",
      "learning_rate: 0.000708501\n",
      "train_loss: 0.070849, valid_loss: 0.051584\n",
      "train_f1: 0.909476, valid_f1: 0.936336\n",
      "--- 28.39198327064514 seconds ---\n",
      "Epoch : 65\n",
      "learning_rate: 0.000697870\n",
      "train_loss: 0.070783, valid_loss: 0.052017\n",
      "train_f1: 0.909799, valid_f1: 0.937178\n",
      "--- 28.33525013923645 seconds ---\n",
      "Epoch : 66\n",
      "learning_rate: 0.000687132\n",
      "train_loss: 0.070065, valid_loss: 0.051438\n",
      "train_f1: 0.910111, valid_f1: 0.936976\n",
      "--- 28.394565105438232 seconds ---\n",
      "Epoch : 67\n",
      "learning_rate: 0.000676292\n",
      "train_loss: 0.070067, valid_loss: 0.051303\n",
      "train_f1: 0.910110, valid_f1: 0.937256\n",
      "--- 28.43801188468933 seconds ---\n",
      "Epoch : 68\n",
      "learning_rate: 0.000665357\n",
      "train_loss: 0.070043, valid_loss: 0.051383\n",
      "train_f1: 0.910298, valid_f1: 0.936666\n",
      "--- 28.346657276153564 seconds ---\n",
      "Epoch : 69\n",
      "learning_rate: 0.000654333\n",
      "train_loss: 0.069837, valid_loss: 0.051399\n",
      "train_f1: 0.910682, valid_f1: 0.936881\n",
      "--- 28.37888240814209 seconds ---\n",
      "Epoch : 70\n",
      "learning_rate: 0.000643225\n",
      "train_loss: 0.069864, valid_loss: 0.051251\n",
      "train_f1: 0.911120, valid_f1: 0.936692\n",
      "--- 28.31666874885559 seconds ---\n",
      "Epoch : 71\n",
      "learning_rate: 0.000632039\n",
      "train_loss: 0.069354, valid_loss: 0.051376\n",
      "train_f1: 0.911264, valid_f1: 0.937052\n",
      "--- 28.266490697860718 seconds ---\n",
      "Epoch : 72\n",
      "learning_rate: 0.000620782\n",
      "train_loss: 0.068912, valid_loss: 0.051208\n",
      "train_f1: 0.911512, valid_f1: 0.936313\n",
      "--- 28.136603116989136 seconds ---\n",
      "Epoch : 73\n",
      "learning_rate: 0.000609459\n",
      "train_loss: 0.068647, valid_loss: 0.051232\n",
      "train_f1: 0.911851, valid_f1: 0.936821\n",
      "--- 28.145689487457275 seconds ---\n",
      "Epoch : 74\n",
      "learning_rate: 0.000598077\n",
      "train_loss: 0.068614, valid_loss: 0.050857\n",
      "train_f1: 0.912368, valid_f1: 0.937350\n",
      "--- 28.224616050720215 seconds ---\n",
      "Epoch : 75\n",
      "learning_rate: 0.000586642\n",
      "train_loss: 0.068552, valid_loss: 0.050829\n",
      "train_f1: 0.912277, valid_f1: 0.937285\n",
      "--- 28.14636492729187 seconds ---\n",
      "Epoch : 76\n",
      "learning_rate: 0.000575160\n",
      "train_loss: 0.068059, valid_loss: 0.051056\n",
      "train_f1: 0.912436, valid_f1: 0.936775\n",
      "--- 28.12609314918518 seconds ---\n",
      "Epoch : 77\n",
      "learning_rate: 0.000563638\n",
      "train_loss: 0.067667, valid_loss: 0.050870\n",
      "train_f1: 0.912970, valid_f1: 0.937138\n",
      "--- 28.08692741394043 seconds ---\n",
      "Epoch : 78\n",
      "learning_rate: 0.000552081\n",
      "train_loss: 0.067285, valid_loss: 0.050797\n",
      "train_f1: 0.913338, valid_f1: 0.937323\n",
      "--- 28.04016637802124 seconds ---\n",
      "Epoch : 79\n",
      "learning_rate: 0.000540495\n",
      "train_loss: 0.067498, valid_loss: 0.050903\n",
      "train_f1: 0.913244, valid_f1: 0.937238\n",
      "--- 28.30162811279297 seconds ---\n",
      "Epoch : 80\n",
      "learning_rate: 0.000528888\n",
      "train_loss: 0.067319, valid_loss: 0.050610\n",
      "train_f1: 0.913880, valid_f1: 0.937571\n",
      "--- 28.276103258132935 seconds ---\n",
      "Epoch : 81\n",
      "learning_rate: 0.000517265\n",
      "train_loss: 0.066950, valid_loss: 0.050602\n",
      "train_f1: 0.913952, valid_f1: 0.937302\n",
      "--- 28.16841149330139 seconds ---\n",
      "Epoch : 82\n",
      "learning_rate: 0.000505633\n",
      "train_loss: 0.066583, valid_loss: 0.050602\n",
      "train_f1: 0.913911, valid_f1: 0.937318\n",
      "--- 28.139825344085693 seconds ---\n",
      "Epoch : 83\n",
      "learning_rate: 0.000493998\n",
      "train_loss: 0.066655, valid_loss: 0.050485\n",
      "train_f1: 0.914381, valid_f1: 0.937438\n",
      "--- 28.01313281059265 seconds ---\n",
      "Epoch : 84\n",
      "learning_rate: 0.000482366\n",
      "train_loss: 0.066358, valid_loss: 0.050682\n",
      "train_f1: 0.914367, valid_f1: 0.936671\n",
      "--- 28.102471828460693 seconds ---\n",
      "Epoch : 85\n",
      "learning_rate: 0.000470743\n",
      "train_loss: 0.066555, valid_loss: 0.050517\n",
      "train_f1: 0.914939, valid_f1: 0.937122\n",
      "--- 28.494905471801758 seconds ---\n",
      "Epoch : 86\n",
      "learning_rate: 0.000459137\n",
      "train_loss: 0.066175, valid_loss: 0.050449\n",
      "train_f1: 0.915196, valid_f1: 0.937590\n",
      "--- 28.180624961853027 seconds ---\n",
      "Epoch : 87\n",
      "learning_rate: 0.000447552\n",
      "train_loss: 0.066274, valid_loss: 0.050546\n",
      "train_f1: 0.914884, valid_f1: 0.937040\n",
      "--- 28.34417724609375 seconds ---\n",
      "Epoch : 88\n",
      "learning_rate: 0.000435996\n",
      "train_loss: 0.066096, valid_loss: 0.050515\n",
      "train_f1: 0.914982, valid_f1: 0.937310\n",
      "--- 28.391709089279175 seconds ---\n",
      "Epoch : 89\n",
      "learning_rate: 0.000424475\n",
      "train_loss: 0.065867, valid_loss: 0.050348\n",
      "train_f1: 0.915054, valid_f1: 0.937482\n",
      "--- 28.35716938972473 seconds ---\n",
      "Epoch : 90\n",
      "learning_rate: 0.000412994\n",
      "train_loss: 0.065775, valid_loss: 0.050359\n",
      "train_f1: 0.915426, valid_f1: 0.937067\n",
      "--- 28.075275897979736 seconds ---\n",
      "Epoch : 91\n",
      "learning_rate: 0.000401561\n",
      "train_loss: 0.066291, valid_loss: 0.050373\n",
      "train_f1: 0.915088, valid_f1: 0.937584\n",
      "--- 28.13736057281494 seconds ---\n",
      "Epoch : 92\n",
      "learning_rate: 0.000390181\n",
      "train_loss: 0.065830, valid_loss: 0.050488\n",
      "train_f1: 0.915431, valid_f1: 0.936861\n",
      "--- 28.24900460243225 seconds ---\n",
      "Epoch : 93\n",
      "learning_rate: 0.000378860\n",
      "train_loss: 0.064984, valid_loss: 0.050293\n",
      "train_f1: 0.915917, valid_f1: 0.937819\n",
      "--- 28.10086441040039 seconds ---\n",
      "Epoch : 94\n",
      "learning_rate: 0.000367605\n",
      "train_loss: 0.065062, valid_loss: 0.050214\n",
      "train_f1: 0.915904, valid_f1: 0.937608\n",
      "--- 28.38278889656067 seconds ---\n",
      "Epoch : 95\n",
      "learning_rate: 0.000356422\n",
      "train_loss: 0.064993, valid_loss: 0.050296\n",
      "train_f1: 0.916027, valid_f1: 0.937151\n",
      "--- 28.208046674728394 seconds ---\n",
      "Epoch : 96\n",
      "learning_rate: 0.000345316\n",
      "train_loss: 0.065161, valid_loss: 0.050371\n",
      "train_f1: 0.916180, valid_f1: 0.936799\n",
      "--- 28.325637102127075 seconds ---\n",
      "Epoch : 97\n",
      "learning_rate: 0.000334294\n",
      "train_loss: 0.064846, valid_loss: 0.050200\n",
      "train_f1: 0.916337, valid_f1: 0.937400\n",
      "--- 28.216441869735718 seconds ---\n",
      "Epoch : 98\n",
      "learning_rate: 0.000323362\n",
      "train_loss: 0.064446, valid_loss: 0.050075\n",
      "train_f1: 0.916441, valid_f1: 0.937338\n",
      "--- 28.264878749847412 seconds ---\n",
      "Epoch : 99\n",
      "learning_rate: 0.000312526\n",
      "train_loss: 0.064281, valid_loss: 0.050131\n",
      "train_f1: 0.916894, valid_f1: 0.937235\n",
      "--- 28.327322006225586 seconds ---\n",
      "Epoch : 100\n",
      "learning_rate: 0.000301791\n",
      "train_loss: 0.064031, valid_loss: 0.050110\n",
      "train_f1: 0.916322, valid_f1: 0.937319\n",
      "--- 28.32685160636902 seconds ---\n",
      "Epoch : 101\n",
      "learning_rate: 0.000291163\n",
      "train_loss: 0.064317, valid_loss: 0.050195\n",
      "train_f1: 0.916842, valid_f1: 0.937109\n",
      "--- 28.460456371307373 seconds ---\n",
      "Epoch : 102\n",
      "learning_rate: 0.000280649\n",
      "train_loss: 0.064067, valid_loss: 0.050001\n",
      "train_f1: 0.917067, valid_f1: 0.937080\n",
      "--- 28.340109825134277 seconds ---\n",
      "Epoch : 103\n",
      "learning_rate: 0.000270253\n",
      "train_loss: 0.064040, valid_loss: 0.050076\n",
      "train_f1: 0.917268, valid_f1: 0.937436\n",
      "--- 28.29558825492859 seconds ---\n",
      "Epoch : 104\n",
      "learning_rate: 0.000259981\n",
      "train_loss: 0.063630, valid_loss: 0.049859\n",
      "train_f1: 0.917291, valid_f1: 0.937151\n",
      "--- 28.27950406074524 seconds ---\n",
      "Epoch : 105\n",
      "learning_rate: 0.000249840\n",
      "train_loss: 0.063987, valid_loss: 0.049916\n",
      "train_f1: 0.917164, valid_f1: 0.936709\n",
      "--- 28.33207082748413 seconds ---\n",
      "Epoch : 106\n",
      "learning_rate: 0.000239834\n",
      "train_loss: 0.063825, valid_loss: 0.049689\n",
      "train_f1: 0.917319, valid_f1: 0.937655\n",
      "--- 28.0346782207489 seconds ---\n",
      "Epoch : 107\n",
      "learning_rate: 0.000229969\n",
      "train_loss: 0.063925, valid_loss: 0.049852\n",
      "train_f1: 0.917468, valid_f1: 0.937368\n",
      "--- 28.470407485961914 seconds ---\n",
      "Epoch : 108\n",
      "learning_rate: 0.000220251\n",
      "train_loss: 0.063405, valid_loss: 0.049950\n",
      "train_f1: 0.917870, valid_f1: 0.936942\n",
      "--- 28.443098545074463 seconds ---\n",
      "Epoch : 109\n",
      "learning_rate: 0.000210683\n",
      "train_loss: 0.063672, valid_loss: 0.049874\n",
      "train_f1: 0.917793, valid_f1: 0.937363\n",
      "--- 28.253233194351196 seconds ---\n",
      "Epoch : 110\n",
      "learning_rate: 0.000201273\n",
      "train_loss: 0.063812, valid_loss: 0.049727\n",
      "train_f1: 0.917909, valid_f1: 0.937559\n",
      "--- 28.487775325775146 seconds ---\n",
      "Epoch : 111\n",
      "learning_rate: 0.000192024\n",
      "train_loss: 0.063425, valid_loss: 0.049768\n",
      "train_f1: 0.917983, valid_f1: 0.937421\n",
      "--- 28.291126251220703 seconds ---\n",
      "Epoch : 112\n",
      "learning_rate: 0.000182942\n",
      "train_loss: 0.062982, valid_loss: 0.049735\n",
      "train_f1: 0.917746, valid_f1: 0.937523\n",
      "--- 28.317151308059692 seconds ---\n",
      "Epoch : 113\n",
      "learning_rate: 0.000174031\n",
      "train_loss: 0.062950, valid_loss: 0.049652\n",
      "train_f1: 0.918168, valid_f1: 0.937654\n",
      "Early Stopping...\n",
      "Best Val Score: 0.937819\n",
      "Fold : 4\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "train_loss: 2.735114, valid_loss: 2.142214\n",
      "train_f1: 0.031017, valid_f1: 0.017714\n",
      "--- 28.465371131896973 seconds ---\n",
      "Epoch : 1\n",
      "learning_rate: 0.000011938\n",
      "train_loss: 2.459789, valid_loss: 1.944246\n",
      "train_f1: 0.035025, valid_f1: 0.016902\n",
      "--- 28.377495050430298 seconds ---\n",
      "Epoch : 2\n",
      "learning_rate: 0.000044274\n",
      "train_loss: 2.054864, valid_loss: 1.710972\n",
      "train_f1: 0.049249, valid_f1: 0.022403\n",
      "--- 28.19458246231079 seconds ---\n",
      "Epoch : 3\n",
      "learning_rate: 0.000096592\n",
      "train_loss: 1.685837, valid_loss: 1.217099\n",
      "train_f1: 0.076954, valid_f1: 0.076705\n",
      "--- 28.533271074295044 seconds ---\n",
      "Epoch : 4\n",
      "learning_rate: 0.000166599\n",
      "train_loss: 1.231202, valid_loss: 0.791571\n",
      "train_f1: 0.102263, valid_f1: 0.066807\n",
      "--- 28.356582641601562 seconds ---\n",
      "Epoch : 5\n",
      "learning_rate: 0.000251230\n",
      "train_loss: 0.940420, valid_loss: 0.643858\n",
      "train_f1: 0.143873, valid_f1: 0.159331\n",
      "--- 28.765498638153076 seconds ---\n",
      "Epoch : 6\n",
      "learning_rate: 0.000346779\n",
      "train_loss: 0.765636, valid_loss: 0.511628\n",
      "train_f1: 0.209348, valid_f1: 0.337233\n",
      "--- 28.481524229049683 seconds ---\n",
      "Epoch : 7\n",
      "learning_rate: 0.000449060\n",
      "train_loss: 0.621465, valid_loss: 0.405756\n",
      "train_f1: 0.283633, valid_f1: 0.414059\n",
      "--- 28.459189891815186 seconds ---\n",
      "Epoch : 8\n",
      "learning_rate: 0.000553594\n",
      "train_loss: 0.517615, valid_loss: 0.350855\n",
      "train_f1: 0.341660, valid_f1: 0.441140\n",
      "--- 28.434074640274048 seconds ---\n",
      "Epoch : 9\n",
      "learning_rate: 0.000655802\n",
      "train_loss: 0.447493, valid_loss: 0.305444\n",
      "train_f1: 0.389377, valid_f1: 0.559347\n",
      "--- 28.42378067970276 seconds ---\n",
      "Epoch : 10\n",
      "learning_rate: 0.000751209\n",
      "train_loss: 0.385438, valid_loss: 0.244627\n",
      "train_f1: 0.445307, valid_f1: 0.531215\n",
      "--- 28.352814197540283 seconds ---\n",
      "Epoch : 11\n",
      "learning_rate: 0.000835636\n",
      "train_loss: 0.326607, valid_loss: 0.187242\n",
      "train_f1: 0.518458, valid_f1: 0.654519\n",
      "--- 28.232393264770508 seconds ---\n",
      "Epoch : 12\n",
      "learning_rate: 0.000905384\n",
      "train_loss: 0.277398, valid_loss: 0.152572\n",
      "train_f1: 0.582086, valid_f1: 0.775108\n",
      "--- 28.36916470527649 seconds ---\n",
      "Epoch : 13\n",
      "learning_rate: 0.000957400\n",
      "train_loss: 0.239294, valid_loss: 0.130631\n",
      "train_f1: 0.627944, valid_f1: 0.802059\n",
      "--- 28.412834882736206 seconds ---\n",
      "Epoch : 14\n",
      "learning_rate: 0.000989405\n",
      "train_loss: 0.211437, valid_loss: 0.118686\n",
      "train_f1: 0.661130, valid_f1: 0.812343\n",
      "--- 28.388144969940186 seconds ---\n",
      "Epoch : 15\n",
      "learning_rate: 0.001000000\n",
      "train_loss: 0.191991, valid_loss: 0.108886\n",
      "train_f1: 0.690758, valid_f1: 0.807029\n",
      "--- 28.233534574508667 seconds ---\n",
      "Epoch : 16\n",
      "learning_rate: 0.000999860\n",
      "train_loss: 0.176545, valid_loss: 0.095883\n",
      "train_f1: 0.715096, valid_f1: 0.828342\n",
      "--- 28.419129371643066 seconds ---\n",
      "Epoch : 17\n",
      "learning_rate: 0.000999450\n",
      "train_loss: 0.162944, valid_loss: 0.088722\n",
      "train_f1: 0.739516, valid_f1: 0.819922\n",
      "--- 28.403977394104004 seconds ---\n",
      "Epoch : 18\n",
      "learning_rate: 0.000998769\n",
      "train_loss: 0.152608, valid_loss: 0.079943\n",
      "train_f1: 0.758026, valid_f1: 0.839340\n",
      "--- 28.72870969772339 seconds ---\n",
      "Epoch : 19\n",
      "learning_rate: 0.000997818\n",
      "train_loss: 0.142915, valid_loss: 0.075909\n",
      "train_f1: 0.772443, valid_f1: 0.843374\n",
      "--- 28.332754135131836 seconds ---\n",
      "Epoch : 20\n",
      "learning_rate: 0.000996598\n",
      "train_loss: 0.134533, valid_loss: 0.072632\n",
      "train_f1: 0.783626, valid_f1: 0.844686\n",
      "--- 28.110494136810303 seconds ---\n",
      "Epoch : 21\n",
      "learning_rate: 0.000995108\n",
      "train_loss: 0.128513, valid_loss: 0.071550\n",
      "train_f1: 0.792134, valid_f1: 0.842199\n",
      "--- 28.632856130599976 seconds ---\n",
      "Epoch : 22\n",
      "learning_rate: 0.000993351\n",
      "train_loss: 0.123588, valid_loss: 0.069349\n",
      "train_f1: 0.799688, valid_f1: 0.845363\n",
      "--- 28.289414644241333 seconds ---\n",
      "Epoch : 23\n",
      "learning_rate: 0.000991326\n",
      "train_loss: 0.118690, valid_loss: 0.068228\n",
      "train_f1: 0.805037, valid_f1: 0.846034\n",
      "--- 28.332343578338623 seconds ---\n",
      "Epoch : 24\n",
      "learning_rate: 0.000989035\n",
      "train_loss: 0.115347, valid_loss: 0.067712\n",
      "train_f1: 0.810749, valid_f1: 0.845982\n",
      "--- 28.155434608459473 seconds ---\n",
      "Epoch : 25\n",
      "learning_rate: 0.000986480\n",
      "train_loss: 0.112410, valid_loss: 0.066703\n",
      "train_f1: 0.815698, valid_f1: 0.845100\n",
      "--- 28.14067792892456 seconds ---\n",
      "Epoch : 26\n",
      "learning_rate: 0.000983661\n",
      "train_loss: 0.108821, valid_loss: 0.064804\n",
      "train_f1: 0.821044, valid_f1: 0.846831\n",
      "--- 28.28962755203247 seconds ---\n",
      "Epoch : 27\n",
      "learning_rate: 0.000980580\n",
      "train_loss: 0.107510, valid_loss: 0.065746\n",
      "train_f1: 0.825152, valid_f1: 0.846756\n",
      "--- 28.41313123703003 seconds ---\n",
      "Epoch : 28\n",
      "learning_rate: 0.000977239\n",
      "train_loss: 0.105942, valid_loss: 0.063302\n",
      "train_f1: 0.830302, valid_f1: 0.848874\n",
      "--- 28.541845083236694 seconds ---\n",
      "Epoch : 29\n",
      "learning_rate: 0.000973639\n",
      "train_loss: 0.101926, valid_loss: 0.062594\n",
      "train_f1: 0.836112, valid_f1: 0.859201\n",
      "--- 28.644834995269775 seconds ---\n",
      "Epoch : 30\n",
      "learning_rate: 0.000969783\n",
      "train_loss: 0.100765, valid_loss: 0.063049\n",
      "train_f1: 0.840056, valid_f1: 0.881167\n",
      "--- 28.451242923736572 seconds ---\n",
      "Epoch : 31\n",
      "learning_rate: 0.000965673\n",
      "train_loss: 0.099161, valid_loss: 0.061861\n",
      "train_f1: 0.845382, valid_f1: 0.885302\n",
      "--- 28.306994676589966 seconds ---\n",
      "Epoch : 32\n",
      "learning_rate: 0.000961310\n",
      "train_loss: 0.097141, valid_loss: 0.060950\n",
      "train_f1: 0.850079, valid_f1: 0.909591\n",
      "--- 28.282258987426758 seconds ---\n",
      "Epoch : 33\n",
      "learning_rate: 0.000956698\n",
      "train_loss: 0.096389, valid_loss: 0.060259\n",
      "train_f1: 0.853862, valid_f1: 0.917551\n",
      "--- 28.33525514602661 seconds ---\n",
      "Epoch : 34\n",
      "learning_rate: 0.000951838\n",
      "train_loss: 0.094362, valid_loss: 0.059448\n",
      "train_f1: 0.858866, valid_f1: 0.922302\n",
      "--- 28.314918279647827 seconds ---\n",
      "Epoch : 35\n",
      "learning_rate: 0.000946733\n",
      "train_loss: 0.093202, valid_loss: 0.059513\n",
      "train_f1: 0.862343, valid_f1: 0.925848\n",
      "--- 28.231654167175293 seconds ---\n",
      "Epoch : 36\n",
      "learning_rate: 0.000941387\n",
      "train_loss: 0.091711, valid_loss: 0.058983\n",
      "train_f1: 0.866224, valid_f1: 0.927233\n",
      "--- 28.293957710266113 seconds ---\n",
      "Epoch : 37\n",
      "learning_rate: 0.000935802\n",
      "train_loss: 0.090601, valid_loss: 0.057968\n",
      "train_f1: 0.869610, valid_f1: 0.930170\n",
      "--- 28.299843311309814 seconds ---\n",
      "Epoch : 38\n",
      "learning_rate: 0.000929980\n",
      "train_loss: 0.089433, valid_loss: 0.057646\n",
      "train_f1: 0.871924, valid_f1: 0.933456\n",
      "--- 28.449617862701416 seconds ---\n",
      "Epoch : 39\n",
      "learning_rate: 0.000923926\n",
      "train_loss: 0.088380, valid_loss: 0.057882\n",
      "train_f1: 0.875248, valid_f1: 0.928116\n",
      "--- 28.348878383636475 seconds ---\n",
      "Epoch : 40\n",
      "learning_rate: 0.000917642\n",
      "train_loss: 0.087577, valid_loss: 0.056558\n",
      "train_f1: 0.878069, valid_f1: 0.934142\n",
      "--- 28.436539888381958 seconds ---\n",
      "Epoch : 41\n",
      "learning_rate: 0.000911132\n",
      "train_loss: 0.086853, valid_loss: 0.056612\n",
      "train_f1: 0.880789, valid_f1: 0.931879\n",
      "--- 28.46006727218628 seconds ---\n",
      "Epoch : 42\n",
      "learning_rate: 0.000904400\n",
      "train_loss: 0.085461, valid_loss: 0.055892\n",
      "train_f1: 0.883190, valid_f1: 0.935139\n",
      "--- 28.261936902999878 seconds ---\n",
      "Epoch : 43\n",
      "learning_rate: 0.000897448\n",
      "train_loss: 0.084883, valid_loss: 0.055438\n",
      "train_f1: 0.885969, valid_f1: 0.935349\n",
      "--- 28.44796657562256 seconds ---\n",
      "Epoch : 44\n",
      "learning_rate: 0.000890282\n",
      "train_loss: 0.083492, valid_loss: 0.055451\n",
      "train_f1: 0.887575, valid_f1: 0.933894\n",
      "--- 28.356680154800415 seconds ---\n",
      "Epoch : 45\n",
      "learning_rate: 0.000882903\n",
      "train_loss: 0.082597, valid_loss: 0.055639\n",
      "train_f1: 0.889845, valid_f1: 0.933499\n",
      "--- 28.379587650299072 seconds ---\n",
      "Epoch : 46\n",
      "learning_rate: 0.000875318\n",
      "train_loss: 0.082727, valid_loss: 0.054362\n",
      "train_f1: 0.891197, valid_f1: 0.935897\n",
      "--- 28.376099586486816 seconds ---\n",
      "Epoch : 47\n",
      "learning_rate: 0.000867529\n",
      "train_loss: 0.081117, valid_loss: 0.053807\n",
      "train_f1: 0.892205, valid_f1: 0.935787\n",
      "--- 28.43265390396118 seconds ---\n",
      "Epoch : 48\n",
      "learning_rate: 0.000859542\n",
      "train_loss: 0.080761, valid_loss: 0.054181\n",
      "train_f1: 0.894540, valid_f1: 0.935341\n",
      "--- 28.559602737426758 seconds ---\n",
      "Epoch : 49\n",
      "learning_rate: 0.000851359\n",
      "train_loss: 0.079867, valid_loss: 0.053796\n",
      "train_f1: 0.896222, valid_f1: 0.935800\n",
      "--- 28.396538972854614 seconds ---\n",
      "Epoch : 50\n",
      "learning_rate: 0.000842986\n",
      "train_loss: 0.079257, valid_loss: 0.054089\n",
      "train_f1: 0.897273, valid_f1: 0.934644\n",
      "--- 28.305564880371094 seconds ---\n",
      "Epoch : 51\n",
      "learning_rate: 0.000834428\n",
      "train_loss: 0.078489, valid_loss: 0.053171\n",
      "train_f1: 0.898225, valid_f1: 0.936282\n",
      "--- 28.09808373451233 seconds ---\n",
      "Epoch : 52\n",
      "learning_rate: 0.000825689\n",
      "train_loss: 0.078463, valid_loss: 0.053161\n",
      "train_f1: 0.899176, valid_f1: 0.936353\n",
      "--- 28.155274629592896 seconds ---\n",
      "Epoch : 53\n",
      "learning_rate: 0.000816773\n",
      "train_loss: 0.077432, valid_loss: 0.053082\n",
      "train_f1: 0.898822, valid_f1: 0.936285\n",
      "--- 28.071399688720703 seconds ---\n",
      "Epoch : 54\n",
      "learning_rate: 0.000807685\n",
      "train_loss: 0.076747, valid_loss: 0.053333\n",
      "train_f1: 0.901262, valid_f1: 0.935208\n",
      "--- 28.184356212615967 seconds ---\n",
      "Epoch : 55\n",
      "learning_rate: 0.000798431\n",
      "train_loss: 0.076944, valid_loss: 0.052957\n",
      "train_f1: 0.901596, valid_f1: 0.935612\n",
      "--- 28.086294174194336 seconds ---\n",
      "Epoch : 56\n",
      "learning_rate: 0.000789015\n",
      "train_loss: 0.076260, valid_loss: 0.052692\n",
      "train_f1: 0.901977, valid_f1: 0.936196\n",
      "--- 28.18394923210144 seconds ---\n",
      "Epoch : 57\n",
      "learning_rate: 0.000779443\n",
      "train_loss: 0.075611, valid_loss: 0.052301\n",
      "train_f1: 0.903060, valid_f1: 0.936152\n",
      "--- 28.144099712371826 seconds ---\n",
      "Epoch : 58\n",
      "learning_rate: 0.000769720\n",
      "train_loss: 0.074735, valid_loss: 0.052319\n",
      "train_f1: 0.903875, valid_f1: 0.936362\n",
      "--- 28.154600620269775 seconds ---\n",
      "Epoch : 59\n",
      "learning_rate: 0.000759850\n",
      "train_loss: 0.074744, valid_loss: 0.052313\n",
      "train_f1: 0.904513, valid_f1: 0.936062\n",
      "--- 28.182783126831055 seconds ---\n",
      "Epoch : 60\n",
      "learning_rate: 0.000749840\n",
      "train_loss: 0.079311, valid_loss: 0.052143\n",
      "train_f1: 0.895547, valid_f1: 0.936557\n",
      "--- 28.240784168243408 seconds ---\n",
      "Epoch : 61\n",
      "learning_rate: 0.000739695\n",
      "train_loss: 0.074440, valid_loss: 0.052021\n",
      "train_f1: 0.905383, valid_f1: 0.935920\n",
      "--- 28.432600736618042 seconds ---\n",
      "Epoch : 62\n",
      "learning_rate: 0.000729419\n",
      "train_loss: 0.073479, valid_loss: 0.051810\n",
      "train_f1: 0.905327, valid_f1: 0.936415\n",
      "--- 28.23468852043152 seconds ---\n",
      "Epoch : 63\n",
      "learning_rate: 0.000719020\n",
      "train_loss: 0.073169, valid_loss: 0.051864\n",
      "train_f1: 0.906213, valid_f1: 0.936365\n",
      "--- 28.50357675552368 seconds ---\n",
      "Epoch : 64\n",
      "learning_rate: 0.000708501\n",
      "train_loss: 0.072600, valid_loss: 0.052009\n",
      "train_f1: 0.906817, valid_f1: 0.936000\n",
      "--- 28.384158611297607 seconds ---\n",
      "Epoch : 65\n",
      "learning_rate: 0.000697870\n",
      "train_loss: 0.072895, valid_loss: 0.051936\n",
      "train_f1: 0.906516, valid_f1: 0.936555\n",
      "--- 28.410200595855713 seconds ---\n",
      "Epoch : 66\n",
      "learning_rate: 0.000687132\n",
      "train_loss: 0.072909, valid_loss: 0.051843\n",
      "train_f1: 0.906648, valid_f1: 0.936573\n",
      "--- 28.303056478500366 seconds ---\n",
      "Epoch : 67\n",
      "learning_rate: 0.000676292\n",
      "train_loss: 0.071355, valid_loss: 0.051779\n",
      "train_f1: 0.907753, valid_f1: 0.936619\n",
      "--- 28.145541429519653 seconds ---\n",
      "Epoch : 68\n",
      "learning_rate: 0.000665357\n",
      "train_loss: 0.070976, valid_loss: 0.051356\n",
      "train_f1: 0.908050, valid_f1: 0.936716\n",
      "--- 28.186190843582153 seconds ---\n",
      "Epoch : 69\n",
      "learning_rate: 0.000654333\n",
      "train_loss: 0.071316, valid_loss: 0.051486\n",
      "train_f1: 0.908235, valid_f1: 0.936757\n",
      "--- 28.207916736602783 seconds ---\n",
      "Epoch : 70\n",
      "learning_rate: 0.000643225\n",
      "train_loss: 0.070979, valid_loss: 0.051603\n",
      "train_f1: 0.908916, valid_f1: 0.936530\n",
      "--- 28.189817667007446 seconds ---\n",
      "Epoch : 71\n",
      "learning_rate: 0.000632039\n",
      "train_loss: 0.071310, valid_loss: 0.051531\n",
      "train_f1: 0.908681, valid_f1: 0.936369\n",
      "--- 28.284997701644897 seconds ---\n",
      "Epoch : 72\n",
      "learning_rate: 0.000620782\n",
      "train_loss: 0.070194, valid_loss: 0.051584\n",
      "train_f1: 0.909241, valid_f1: 0.935691\n",
      "--- 28.13642907142639 seconds ---\n",
      "Epoch : 73\n",
      "learning_rate: 0.000609459\n",
      "train_loss: 0.069777, valid_loss: 0.051471\n",
      "train_f1: 0.909758, valid_f1: 0.936475\n",
      "--- 28.29168462753296 seconds ---\n",
      "Epoch : 74\n",
      "learning_rate: 0.000598077\n",
      "train_loss: 0.070376, valid_loss: 0.051579\n",
      "train_f1: 0.909911, valid_f1: 0.936069\n",
      "--- 28.263407468795776 seconds ---\n",
      "Epoch : 75\n",
      "learning_rate: 0.000586642\n",
      "train_loss: 0.069800, valid_loss: 0.051483\n",
      "train_f1: 0.909967, valid_f1: 0.935974\n",
      "--- 28.26977825164795 seconds ---\n",
      "Epoch : 76\n",
      "learning_rate: 0.000575160\n",
      "train_loss: 0.069355, valid_loss: 0.051270\n",
      "train_f1: 0.910635, valid_f1: 0.936687\n",
      "--- 28.41483759880066 seconds ---\n",
      "Epoch : 77\n",
      "learning_rate: 0.000563638\n",
      "train_loss: 0.069204, valid_loss: 0.051074\n",
      "train_f1: 0.910804, valid_f1: 0.936746\n",
      "--- 28.26703953742981 seconds ---\n",
      "Epoch : 78\n",
      "learning_rate: 0.000552081\n",
      "train_loss: 0.069102, valid_loss: 0.051207\n",
      "train_f1: 0.910761, valid_f1: 0.936600\n",
      "--- 28.22849178314209 seconds ---\n",
      "Epoch : 79\n",
      "learning_rate: 0.000540495\n",
      "train_loss: 0.068356, valid_loss: 0.051336\n",
      "train_f1: 0.911214, valid_f1: 0.936771\n",
      "--- 28.452385663986206 seconds ---\n",
      "Epoch : 80\n",
      "learning_rate: 0.000528888\n",
      "train_loss: 0.068204, valid_loss: 0.051219\n",
      "train_f1: 0.911566, valid_f1: 0.936854\n",
      "--- 28.18989086151123 seconds ---\n",
      "Epoch : 81\n",
      "learning_rate: 0.000517265\n",
      "train_loss: 0.067929, valid_loss: 0.051203\n",
      "train_f1: 0.911653, valid_f1: 0.936886\n",
      "--- 28.478958129882812 seconds ---\n",
      "Epoch : 82\n",
      "learning_rate: 0.000505633\n",
      "train_loss: 0.067965, valid_loss: 0.051202\n",
      "train_f1: 0.911938, valid_f1: 0.936283\n",
      "--- 28.301031827926636 seconds ---\n",
      "Epoch : 83\n",
      "learning_rate: 0.000493998\n",
      "train_loss: 0.067757, valid_loss: 0.051510\n",
      "train_f1: 0.912015, valid_f1: 0.935471\n",
      "--- 28.262078285217285 seconds ---\n",
      "Epoch : 84\n",
      "learning_rate: 0.000482366\n",
      "train_loss: 0.067531, valid_loss: 0.050798\n",
      "train_f1: 0.912498, valid_f1: 0.936980\n",
      "--- 28.429335355758667 seconds ---\n",
      "Epoch : 85\n",
      "learning_rate: 0.000470743\n",
      "train_loss: 0.067797, valid_loss: 0.051234\n",
      "train_f1: 0.912013, valid_f1: 0.935950\n",
      "--- 28.248892068862915 seconds ---\n",
      "Epoch : 86\n",
      "learning_rate: 0.000459137\n",
      "train_loss: 0.067112, valid_loss: 0.051888\n",
      "train_f1: 0.912846, valid_f1: 0.934005\n",
      "--- 28.265249967575073 seconds ---\n",
      "Epoch : 87\n",
      "learning_rate: 0.000447552\n",
      "train_loss: 0.066971, valid_loss: 0.051098\n",
      "train_f1: 0.912875, valid_f1: 0.936554\n",
      "--- 28.37519884109497 seconds ---\n",
      "Epoch : 88\n",
      "learning_rate: 0.000435996\n",
      "train_loss: 0.067055, valid_loss: 0.051203\n",
      "train_f1: 0.912692, valid_f1: 0.935598\n",
      "--- 28.34390377998352 seconds ---\n",
      "Epoch : 89\n",
      "learning_rate: 0.000424475\n",
      "train_loss: 0.066319, valid_loss: 0.050785\n",
      "train_f1: 0.913025, valid_f1: 0.936814\n",
      "--- 28.43184542655945 seconds ---\n",
      "Epoch : 90\n",
      "learning_rate: 0.000412994\n",
      "train_loss: 0.066454, valid_loss: 0.050735\n",
      "train_f1: 0.913180, valid_f1: 0.936807\n",
      "--- 28.406944036483765 seconds ---\n",
      "Epoch : 91\n",
      "learning_rate: 0.000401561\n",
      "train_loss: 0.065787, valid_loss: 0.050897\n",
      "train_f1: 0.913682, valid_f1: 0.936635\n",
      "--- 28.40194869041443 seconds ---\n",
      "Epoch : 92\n",
      "learning_rate: 0.000390181\n",
      "train_loss: 0.066456, valid_loss: 0.051099\n",
      "train_f1: 0.913383, valid_f1: 0.935260\n",
      "--- 28.159223079681396 seconds ---\n",
      "Epoch : 93\n",
      "learning_rate: 0.000378860\n",
      "train_loss: 0.066375, valid_loss: 0.051014\n",
      "train_f1: 0.913676, valid_f1: 0.935922\n",
      "--- 28.419320821762085 seconds ---\n",
      "Epoch : 94\n",
      "learning_rate: 0.000367605\n",
      "train_loss: 0.065917, valid_loss: 0.051112\n",
      "train_f1: 0.913754, valid_f1: 0.936718\n",
      "--- 28.386078119277954 seconds ---\n",
      "Epoch : 95\n",
      "learning_rate: 0.000356422\n",
      "train_loss: 0.065779, valid_loss: 0.050724\n",
      "train_f1: 0.913790, valid_f1: 0.936340\n",
      "--- 28.199246168136597 seconds ---\n",
      "Epoch : 96\n",
      "learning_rate: 0.000345316\n",
      "train_loss: 0.065420, valid_loss: 0.050987\n",
      "train_f1: 0.914425, valid_f1: 0.936596\n",
      "--- 28.30405902862549 seconds ---\n",
      "Epoch : 97\n",
      "learning_rate: 0.000334294\n",
      "train_loss: 0.065295, valid_loss: 0.050779\n",
      "train_f1: 0.914131, valid_f1: 0.936576\n",
      "--- 28.32320547103882 seconds ---\n",
      "Epoch : 98\n",
      "learning_rate: 0.000323362\n",
      "train_loss: 0.064972, valid_loss: 0.050785\n",
      "train_f1: 0.913899, valid_f1: 0.936206\n",
      "--- 28.139904260635376 seconds ---\n",
      "Epoch : 99\n",
      "learning_rate: 0.000312526\n",
      "train_loss: 0.065274, valid_loss: 0.050981\n",
      "train_f1: 0.914422, valid_f1: 0.935253\n",
      "--- 28.33202576637268 seconds ---\n",
      "Epoch : 100\n",
      "learning_rate: 0.000301791\n",
      "train_loss: 0.065560, valid_loss: 0.050749\n",
      "train_f1: 0.914937, valid_f1: 0.936512\n",
      "--- 28.42308259010315 seconds ---\n",
      "Epoch : 101\n",
      "learning_rate: 0.000291163\n",
      "train_loss: 0.064504, valid_loss: 0.050461\n",
      "train_f1: 0.914925, valid_f1: 0.936823\n",
      "--- 28.416672706604004 seconds ---\n",
      "Epoch : 102\n",
      "learning_rate: 0.000280649\n",
      "train_loss: 0.064870, valid_loss: 0.050839\n",
      "train_f1: 0.914497, valid_f1: 0.936322\n",
      "--- 28.202919483184814 seconds ---\n",
      "Epoch : 103\n",
      "learning_rate: 0.000270253\n",
      "train_loss: 0.064690, valid_loss: 0.050733\n",
      "train_f1: 0.914973, valid_f1: 0.935858\n",
      "--- 28.308421850204468 seconds ---\n",
      "Epoch : 104\n",
      "learning_rate: 0.000259981\n",
      "train_loss: 0.064550, valid_loss: 0.050944\n",
      "train_f1: 0.915308, valid_f1: 0.935814\n",
      "Early Stopping...\n",
      "Best Val Score: 0.936980\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "for index, (train_index, val_index ) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    \n",
    "    batchsize = 16\n",
    "    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, class_split=0.0, aug_ratio=0.5)\n",
    "    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_preds_iter = np.zeros((2000000, 11))\n",
    "    it = 0\n",
    "    for it in range(1):\n",
    "        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=64, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "    \n",
    "        no_of_epochs = 150\n",
    "        early_stopping = EarlyStopping(patience=20, is_maximize=True, checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n",
    "        criterion = L.FocalLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=0.001, epochs=no_of_epochs,\n",
    "                                                steps_per_epoch=len(train_dataloader))\n",
    "        avg_train_losses, avg_valid_losses = [], [] \n",
    "    \n",
    "    \n",
    "        for epoch in range(no_of_epochs):\n",
    "            start_time = time.time()\n",
    "    \n",
    "            print(\"Epoch : {}\".format(epoch))\n",
    "            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n",
    "            train_losses, valid_losses = [], []\n",
    "    \n",
    "            model.train() # prep model for training\n",
    "            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "            for x, y in train_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(x[:, :trainval.shape[1], :])\n",
    "    \n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                y_ = y.view(-1)\n",
    "    \n",
    "                loss = criterion(predictions_, y_)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                schedular.step()\n",
    "                # record training lossa\n",
    "                train_losses.append(loss.item())\n",
    "    \n",
    "                train_true = torch.cat([train_true, y_], 0)\n",
    "                train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "            model.eval() # prep model for evaluation\n",
    "            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "    \n",
    "                    predictions = model(x[:,:trainval.shape[1],:])\n",
    "                    predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                    y_ = y.view(-1)\n",
    "    \n",
    "                    loss = criterion(predictions_, y_)\n",
    "                    valid_losses.append(loss.item())\n",
    "        \n",
    "                    val_true = torch.cat([val_true, y_], 0)\n",
    "                    val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "            \n",
    "            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "    \n",
    "            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "    \n",
    "            if early_stopping(val_score, model):\n",
    "                print(\"Early Stopping...\")\n",
    "                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n",
    "                break\n",
    "    \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n",
    "        with torch.no_grad():\n",
    "            pred_list = []\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                predictions = model(x[:,:trainval.shape[1],:])\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "\n",
    "                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "       \n",
    "        test_preds_iter += test_preds\n",
    "        test_preds_all += test_preds\n",
    "        if not os.path.exists(\"./predictions/test\"):\n",
    "            os.makedirs(\"./predictions/test\")\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
